{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_gen_manNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_manNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojVYZ7Spzpv",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   Check TF version\n",
        "*    TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q4JaV2ps6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd9fca27-0594-47f8-8d87-58907ece0980"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVhQoAOqGGW",
        "colab_type": "code",
        "outputId": "a773411f-dd4a-4489-9288-a7f70e775fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/F2O/pyF2O"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/F2O/pyF2O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vp4y0TiqKJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692-uM-yqdaF",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9QkSiJqed6",
        "colab_type": "code",
        "outputId": "8324a8aa-0d8f-44ca-c678-9a355864646e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.89.44.202:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 158130094781230494),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2271453469138602302),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8992534795676828576),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5272453975527956165),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15866929967926059721),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4833514300863849293),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8012910824252317233),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10535885469190820447),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8768462604081482841),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4949009508707155949),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 913067740314734122)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefiHZ4qlHA",
        "colab_type": "text"
      },
      "source": [
        "# manNet generator Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwtSzOarVYM",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOro7D1krWYf",
        "colab_type": "code",
        "outputId": "bca74a28-b9a8-4a2b-b6e3-7ba2d4b52578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import mean_absolute_error\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from core.generators import man_net\n",
        "model=man_net()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(lr=0.01), loss=mean_absolute_error)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 8)      4104        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 8)      36872       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 8)      102408      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 8, 24)     0           conv2d_10[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 8, 8, 24)     96          concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 8, 8, 24)     0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 16, 16, 24)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 6)    150         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 6)    1302        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 6)    3606        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 16, 16, 18)   0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 16, 16, 18)   72          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 16, 16, 18)   0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 32, 32, 18)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 4)    76          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 4)    652         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 4)    1804        lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 32, 32, 12)   48          concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 12)   0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 64, 64, 12)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 64, 64, 2)    26          lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 64, 64, 2)    218         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 64, 64, 2)    602         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 64, 64, 6)    0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 64, 64, 6)    24          concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 6)    0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 128, 128, 6)  0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 128, 128, 2)  302         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 128, 128, 2)  590         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 128, 128, 2)  1454        lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 128, 128, 6)  0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 128, 128, 6)  24          concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 128, 128, 6)  0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 128, 128, 3)  165         activation_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 7,789,859\n",
            "Trainable params: 7,789,727\n",
            "Non-trainable params: 132\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fqLlsIcrgJ6",
        "colab_type": "text"
      },
      "source": [
        "## Convert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLuMCkq5rha3",
        "colab_type": "code",
        "outputId": "13a8a4a1-c465-4769-f1a1-e18ca6784a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def convert_model_TPU(model):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "                                           strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                                               tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "                                               )\n",
        "                                           )\n",
        "\n",
        "model=convert_model_TPU(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.89.44.202:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 158130094781230494)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2271453469138602302)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8992534795676828576)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5272453975527956165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15866929967926059721)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4833514300863849293)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8012910824252317233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10535885469190820447)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8768462604081482841)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4949009508707155949)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 913067740314734122)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMWNlaqLNIPY",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "* Load **X_Train_ALL.h5** and **Y_Train_ALL.h5**\n",
        "* Normalize both dataset as float32\n",
        "* Split the data for **validation** and **training**\n",
        "* Drop Batch Reminders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WMdSlCZJPWi",
        "colab_type": "code",
        "outputId": "785f0501-c68d-4ad4-9651-14726b5c34c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from core.utils import readh5\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "class TRAIN_FLAGS:\n",
        "    H5_DIR   = '/content/gdrive/My Drive/PROJECTS/F2O/H5Data/' # @param\n",
        "    BATCH_SIZE      = 128  # @param\n",
        "    NUM_EPOCHS      = 500  # @param\n",
        "    IMAGE_DIM       = 128  # @param\n",
        "    X_IDEN          = 'X_Train_ALL.h5'  # @param\n",
        "    Y_IDEN          = 'Y_Train_ALL.h5'  # @param\n",
        "\n",
        "X=readh5(os.path.join(TRAIN_FLAGS.H5_DIR,TRAIN_FLAGS.X_IDEN))\n",
        "Y=readh5(os.path.join(TRAIN_FLAGS.H5_DIR,TRAIN_FLAGS.Y_IDEN))\n",
        "# Normalize\n",
        "X=X.astype('float32')/255.0\n",
        "Y=Y.astype('float32')/255.0\n",
        "# Test And Validation\n",
        "Xt,Xv,Yt,Yv=train_test_split(X,Y,test_size=0.2)\n",
        "\n",
        "print(\"Before Drop\")\n",
        "print(\"X_train: {}\".format(Xt.shape))\n",
        "print(\"Y_train: {}\".format(Yt.shape))\n",
        "print(\"X_eval: {}\".format(Xv.shape))\n",
        "print(\"Y_eval: {}\".format(Yv.shape))\n",
        "\n",
        "# Drop Batch Residue\n",
        "nb_sig_train=int(Xt.shape[0]/TRAIN_FLAGS.BATCH_SIZE)*TRAIN_FLAGS.BATCH_SIZE \n",
        "nb_sig_eval=int(Xv.shape[0]/TRAIN_FLAGS.BATCH_SIZE)*TRAIN_FLAGS.BATCH_SIZE \n",
        "\n",
        "Xt=Xt[:nb_sig_train,:,:,:]\n",
        "Yt=Yt[:nb_sig_train,:,:,:]\n",
        "Xv=Xv[:nb_sig_eval,:,:,:]\n",
        "Yv=Yv[:nb_sig_eval,:,:,:]\n",
        "\n",
        "print(\"After Drop\")\n",
        "print(\"X_train: {}\".format(Xt.shape))\n",
        "print(\"Y_train: {}\".format(Yt.shape))\n",
        "print(\"X_eval: {}\".format(Xv.shape))\n",
        "print(\"Y_eval: {}\".format(Yv.shape))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Drop\n",
            "X_train: (4480, 128, 128, 3)\n",
            "Y_train: (4480, 128, 128, 3)\n",
            "X_eval: (1120, 128, 128, 3)\n",
            "Y_eval: (1120, 128, 128, 3)\n",
            "After Drop\n",
            "X_train: (4480, 128, 128, 3)\n",
            "Y_train: (4480, 128, 128, 3)\n",
            "X_eval: (1024, 128, 128, 3)\n",
            "Y_eval: (1024, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQXx8YRNSyr",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTUQlweNYYN",
        "colab_type": "code",
        "outputId": "0759b3a9-b142-42c2-97c6-77dbb4a904da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history=model.fit(Xt,Yt,validation_data=(Xv,Yv),epochs=TRAIN_FLAGS.NUM_EPOCHS,batch_size=TRAIN_FLAGS.BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4480 samples, validate on 1024 samples\n",
            "Epoch 1/500\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fcdeba65b70> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 48.515430212020874 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.009999999776482582 {0.01}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4352/4480 [============================>.] - ETA: 2s - loss: 0.1711INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fcde0f4c2b0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 17.90699553489685 secs\n",
            "4480/4480 [==============================] - 117s 26ms/sample - loss: 0.1701 - val_loss: 0.4811\n",
            "Epoch 2/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1231 - val_loss: 0.3822\n",
            "Epoch 3/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1132 - val_loss: 0.1565\n",
            "Epoch 4/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1097 - val_loss: 0.1321\n",
            "Epoch 5/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1069 - val_loss: 0.1743\n",
            "Epoch 6/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1049 - val_loss: 0.1764\n",
            "Epoch 7/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1033 - val_loss: 0.1942\n",
            "Epoch 8/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1017 - val_loss: 0.1540\n",
            "Epoch 9/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.1002 - val_loss: 0.1658\n",
            "Epoch 10/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0991 - val_loss: 0.1305\n",
            "Epoch 11/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0979 - val_loss: 0.1162\n",
            "Epoch 12/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0972 - val_loss: 0.1368\n",
            "Epoch 13/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0958 - val_loss: 0.1161\n",
            "Epoch 14/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0950 - val_loss: 0.1025\n",
            "Epoch 15/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0950 - val_loss: 0.1308\n",
            "Epoch 16/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0934 - val_loss: 0.0993\n",
            "Epoch 17/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0926 - val_loss: 0.1037\n",
            "Epoch 18/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0922 - val_loss: 0.1373\n",
            "Epoch 19/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0912 - val_loss: 0.1638\n",
            "Epoch 20/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0907 - val_loss: 0.1209\n",
            "Epoch 21/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0901 - val_loss: 0.1945\n",
            "Epoch 22/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0896 - val_loss: 0.1165\n",
            "Epoch 23/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0895 - val_loss: 0.1989\n",
            "Epoch 24/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0885 - val_loss: 0.1358\n",
            "Epoch 25/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0878 - val_loss: 0.1092\n",
            "Epoch 26/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0875 - val_loss: 0.2132\n",
            "Epoch 27/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0864 - val_loss: 0.1620\n",
            "Epoch 28/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0867 - val_loss: 0.2130\n",
            "Epoch 29/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0855 - val_loss: 0.1258\n",
            "Epoch 30/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0850 - val_loss: 0.1031\n",
            "Epoch 31/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0850 - val_loss: 0.0870\n",
            "Epoch 32/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0844 - val_loss: 0.1198\n",
            "Epoch 33/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0838 - val_loss: 0.0869\n",
            "Epoch 34/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0835 - val_loss: 0.1577\n",
            "Epoch 35/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0843 - val_loss: 0.1264\n",
            "Epoch 36/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0835 - val_loss: 0.0973\n",
            "Epoch 37/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0829 - val_loss: 0.1193\n",
            "Epoch 38/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0829 - val_loss: 0.1177\n",
            "Epoch 39/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0828 - val_loss: 0.1590\n",
            "Epoch 40/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0815 - val_loss: 0.0888\n",
            "Epoch 41/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0811 - val_loss: 0.0897\n",
            "Epoch 42/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0814 - val_loss: 0.1012\n",
            "Epoch 43/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0815 - val_loss: 0.1094\n",
            "Epoch 44/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0810 - val_loss: 0.0824\n",
            "Epoch 45/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0823 - val_loss: 0.1070\n",
            "Epoch 46/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0810 - val_loss: 0.1157\n",
            "Epoch 47/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0801 - val_loss: 0.0840\n",
            "Epoch 48/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0803 - val_loss: 0.1081\n",
            "Epoch 49/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0804 - val_loss: 0.1085\n",
            "Epoch 50/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0791 - val_loss: 0.0864\n",
            "Epoch 51/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0787 - val_loss: 0.0814\n",
            "Epoch 52/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0794 - val_loss: 0.1017\n",
            "Epoch 53/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0796 - val_loss: 0.1128\n",
            "Epoch 54/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0790 - val_loss: 0.1066\n",
            "Epoch 55/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0784 - val_loss: 0.0989\n",
            "Epoch 56/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0792 - val_loss: 0.1047\n",
            "Epoch 57/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0786 - val_loss: 0.1275\n",
            "Epoch 58/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0781 - val_loss: 0.0966\n",
            "Epoch 59/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0778 - val_loss: 0.0883\n",
            "Epoch 60/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0779 - val_loss: 0.1279\n",
            "Epoch 61/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0775 - val_loss: 0.1256\n",
            "Epoch 62/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0778 - val_loss: 0.1168\n",
            "Epoch 63/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0775 - val_loss: 0.1156\n",
            "Epoch 64/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0765 - val_loss: 0.0781\n",
            "Epoch 65/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0771 - val_loss: 0.0982\n",
            "Epoch 66/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0764 - val_loss: 0.0885\n",
            "Epoch 67/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0772 - val_loss: 0.1247\n",
            "Epoch 68/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0768 - val_loss: 0.0969\n",
            "Epoch 69/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0765 - val_loss: 0.0874\n",
            "Epoch 70/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0758 - val_loss: 0.0829\n",
            "Epoch 71/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0760 - val_loss: 0.0930\n",
            "Epoch 72/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0756 - val_loss: 0.0780\n",
            "Epoch 73/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0756 - val_loss: 0.0855\n",
            "Epoch 74/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0760 - val_loss: 0.1064\n",
            "Epoch 75/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0758 - val_loss: 0.1109\n",
            "Epoch 76/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0755 - val_loss: 0.0802\n",
            "Epoch 77/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0755 - val_loss: 0.1111\n",
            "Epoch 78/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0754 - val_loss: 0.0930\n",
            "Epoch 79/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0755 - val_loss: 0.1041\n",
            "Epoch 80/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0745 - val_loss: 0.0748\n",
            "Epoch 81/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0746 - val_loss: 0.1022\n",
            "Epoch 82/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0748 - val_loss: 0.0813\n",
            "Epoch 83/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0744 - val_loss: 0.0944\n",
            "Epoch 84/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0738 - val_loss: 0.0972\n",
            "Epoch 85/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0743 - val_loss: 0.1005\n",
            "Epoch 86/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0738 - val_loss: 0.0961\n",
            "Epoch 87/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0752 - val_loss: 0.1008\n",
            "Epoch 88/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0737 - val_loss: 0.0749\n",
            "Epoch 89/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0736 - val_loss: 0.0759\n",
            "Epoch 90/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0734 - val_loss: 0.0813\n",
            "Epoch 91/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0739 - val_loss: 0.0745\n",
            "Epoch 92/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0739 - val_loss: 0.0962\n",
            "Epoch 93/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0729 - val_loss: 0.0910\n",
            "Epoch 94/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0735 - val_loss: 0.0868\n",
            "Epoch 95/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0732 - val_loss: 0.0901\n",
            "Epoch 96/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0730 - val_loss: 0.0986\n",
            "Epoch 97/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0730 - val_loss: 0.1067\n",
            "Epoch 98/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0730 - val_loss: 0.0888\n",
            "Epoch 99/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0728 - val_loss: 0.0994\n",
            "Epoch 100/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0723 - val_loss: 0.0912\n",
            "Epoch 101/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0724 - val_loss: 0.0918\n",
            "Epoch 102/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0727 - val_loss: 0.0768\n",
            "Epoch 103/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0724 - val_loss: 0.0747\n",
            "Epoch 104/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0727 - val_loss: 0.0892\n",
            "Epoch 105/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0725 - val_loss: 0.0985\n",
            "Epoch 106/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0731 - val_loss: 0.1280\n",
            "Epoch 107/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0731 - val_loss: 0.1032\n",
            "Epoch 108/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0719 - val_loss: 0.0911\n",
            "Epoch 109/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0718 - val_loss: 0.0946\n",
            "Epoch 110/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0717 - val_loss: 0.0819\n",
            "Epoch 111/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0721 - val_loss: 0.0899\n",
            "Epoch 112/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0716 - val_loss: 0.0790\n",
            "Epoch 113/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0711 - val_loss: 0.0953\n",
            "Epoch 114/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0712 - val_loss: 0.0870\n",
            "Epoch 115/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0715 - val_loss: 0.0723\n",
            "Epoch 116/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0715 - val_loss: 0.0927\n",
            "Epoch 117/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0713 - val_loss: 0.0788\n",
            "Epoch 118/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0709 - val_loss: 0.0725\n",
            "Epoch 119/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0711 - val_loss: 0.0843\n",
            "Epoch 120/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0706 - val_loss: 0.0754\n",
            "Epoch 121/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0704 - val_loss: 0.0708\n",
            "Epoch 122/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0708 - val_loss: 0.0967\n",
            "Epoch 123/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0707 - val_loss: 0.0864\n",
            "Epoch 124/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0708 - val_loss: 0.1158\n",
            "Epoch 125/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0720 - val_loss: 0.1017\n",
            "Epoch 126/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0705 - val_loss: 0.0844\n",
            "Epoch 127/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0701 - val_loss: 0.0706\n",
            "Epoch 128/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0702 - val_loss: 0.0880\n",
            "Epoch 129/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0705 - val_loss: 0.0924\n",
            "Epoch 130/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0704 - val_loss: 0.0886\n",
            "Epoch 131/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0708 - val_loss: 0.0791\n",
            "Epoch 132/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0706 - val_loss: 0.0825\n",
            "Epoch 133/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0707 - val_loss: 0.1659\n",
            "Epoch 134/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0705 - val_loss: 0.0874\n",
            "Epoch 135/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0697 - val_loss: 0.0745\n",
            "Epoch 136/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0701 - val_loss: 0.0718\n",
            "Epoch 137/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0695 - val_loss: 0.0951\n",
            "Epoch 138/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0697 - val_loss: 0.1028\n",
            "Epoch 139/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0694 - val_loss: 0.0744\n",
            "Epoch 140/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0699 - val_loss: 0.0810\n",
            "Epoch 141/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0697 - val_loss: 0.0880\n",
            "Epoch 142/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0695 - val_loss: 0.0912\n",
            "Epoch 143/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0694 - val_loss: 0.1067\n",
            "Epoch 144/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0697 - val_loss: 0.0790\n",
            "Epoch 145/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0695 - val_loss: 0.0974\n",
            "Epoch 146/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0695 - val_loss: 0.0890\n",
            "Epoch 147/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0693 - val_loss: 0.0789\n",
            "Epoch 148/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0700 - val_loss: 0.1055\n",
            "Epoch 149/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0690 - val_loss: 0.0722\n",
            "Epoch 150/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0690 - val_loss: 0.0980\n",
            "Epoch 151/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0687 - val_loss: 0.0725\n",
            "Epoch 152/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0691 - val_loss: 0.0765\n",
            "Epoch 153/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0697 - val_loss: 0.1269\n",
            "Epoch 154/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0691 - val_loss: 0.0844\n",
            "Epoch 155/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0684 - val_loss: 0.0785\n",
            "Epoch 156/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0688 - val_loss: 0.0876\n",
            "Epoch 157/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0684 - val_loss: 0.0770\n",
            "Epoch 158/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0688 - val_loss: 0.0857\n",
            "Epoch 159/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0684 - val_loss: 0.0884\n",
            "Epoch 160/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0690 - val_loss: 0.0815\n",
            "Epoch 161/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0686 - val_loss: 0.1121\n",
            "Epoch 162/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0686 - val_loss: 0.0844\n",
            "Epoch 163/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0684 - val_loss: 0.0737\n",
            "Epoch 164/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0686 - val_loss: 0.0923\n",
            "Epoch 165/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0682 - val_loss: 0.0857\n",
            "Epoch 166/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0687 - val_loss: 0.1031\n",
            "Epoch 167/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0684 - val_loss: 0.0959\n",
            "Epoch 168/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0681 - val_loss: 0.0695\n",
            "Epoch 169/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0680 - val_loss: 0.0967\n",
            "Epoch 170/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0679 - val_loss: 0.0801\n",
            "Epoch 171/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0675 - val_loss: 0.0726\n",
            "Epoch 172/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0681 - val_loss: 0.0832\n",
            "Epoch 173/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0679 - val_loss: 0.0868\n",
            "Epoch 174/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0683 - val_loss: 0.0985\n",
            "Epoch 175/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0679 - val_loss: 0.0955\n",
            "Epoch 176/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0678 - val_loss: 0.0802\n",
            "Epoch 177/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0679 - val_loss: 0.0935\n",
            "Epoch 178/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0678 - val_loss: 0.0808\n",
            "Epoch 179/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0679 - val_loss: 0.0886\n",
            "Epoch 180/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0683 - val_loss: 0.0970\n",
            "Epoch 181/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0675 - val_loss: 0.0963\n",
            "Epoch 182/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0683 - val_loss: 0.0928\n",
            "Epoch 183/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0671 - val_loss: 0.0813\n",
            "Epoch 184/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0674 - val_loss: 0.0729\n",
            "Epoch 185/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0672 - val_loss: 0.0709\n",
            "Epoch 186/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0671 - val_loss: 0.0705\n",
            "Epoch 187/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0674 - val_loss: 0.0819\n",
            "Epoch 188/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0670 - val_loss: 0.0729\n",
            "Epoch 189/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0668 - val_loss: 0.0711\n",
            "Epoch 190/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0669 - val_loss: 0.0686\n",
            "Epoch 191/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0674 - val_loss: 0.0762\n",
            "Epoch 192/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0669 - val_loss: 0.0715\n",
            "Epoch 193/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0666 - val_loss: 0.0721\n",
            "Epoch 194/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0679 - val_loss: 0.0938\n",
            "Epoch 195/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0676 - val_loss: 0.1115\n",
            "Epoch 196/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0681 - val_loss: 0.1038\n",
            "Epoch 197/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0675 - val_loss: 0.0710\n",
            "Epoch 198/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0667 - val_loss: 0.0865\n",
            "Epoch 199/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0667 - val_loss: 0.0726\n",
            "Epoch 200/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0671 - val_loss: 0.0697\n",
            "Epoch 201/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0666 - val_loss: 0.0667\n",
            "Epoch 202/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0667 - val_loss: 0.0784\n",
            "Epoch 203/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0704\n",
            "Epoch 204/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0663 - val_loss: 0.0678\n",
            "Epoch 205/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0668 - val_loss: 0.0827\n",
            "Epoch 206/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0665 - val_loss: 0.0692\n",
            "Epoch 207/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0660 - val_loss: 0.0714\n",
            "Epoch 208/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0666 - val_loss: 0.0676\n",
            "Epoch 209/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0665 - val_loss: 0.0813\n",
            "Epoch 210/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0692\n",
            "Epoch 211/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0666 - val_loss: 0.0758\n",
            "Epoch 212/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0665 - val_loss: 0.0859\n",
            "Epoch 213/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0660 - val_loss: 0.0720\n",
            "Epoch 214/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0665 - val_loss: 0.0860\n",
            "Epoch 215/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0661 - val_loss: 0.0734\n",
            "Epoch 216/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0765\n",
            "Epoch 217/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0665 - val_loss: 0.1136\n",
            "Epoch 218/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0666 - val_loss: 0.0753\n",
            "Epoch 219/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0666 - val_loss: 0.0862\n",
            "Epoch 220/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0659 - val_loss: 0.0779\n",
            "Epoch 221/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0771\n",
            "Epoch 222/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0825\n",
            "Epoch 223/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.1025\n",
            "Epoch 224/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0665 - val_loss: 0.0761\n",
            "Epoch 225/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0668 - val_loss: 0.0768\n",
            "Epoch 226/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0672 - val_loss: 0.0925\n",
            "Epoch 227/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0663 - val_loss: 0.0733\n",
            "Epoch 228/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0656 - val_loss: 0.0742\n",
            "Epoch 229/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0663 - val_loss: 0.0742\n",
            "Epoch 230/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0779\n",
            "Epoch 231/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0654 - val_loss: 0.0664\n",
            "Epoch 232/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0660 - val_loss: 0.0672\n",
            "Epoch 233/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0661 - val_loss: 0.0722\n",
            "Epoch 234/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0658 - val_loss: 0.0706\n",
            "Epoch 235/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0658 - val_loss: 0.0852\n",
            "Epoch 236/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0655 - val_loss: 0.0804\n",
            "Epoch 237/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0656 - val_loss: 0.0843\n",
            "Epoch 238/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0663 - val_loss: 0.0670\n",
            "Epoch 239/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0658 - val_loss: 0.0746\n",
            "Epoch 240/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0717\n",
            "Epoch 241/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0652 - val_loss: 0.0697\n",
            "Epoch 242/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0656 - val_loss: 0.0972\n",
            "Epoch 243/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0661 - val_loss: 0.0838\n",
            "Epoch 244/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0657 - val_loss: 0.0697\n",
            "Epoch 245/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0731\n",
            "Epoch 246/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0653 - val_loss: 0.0688\n",
            "Epoch 247/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0658 - val_loss: 0.0834\n",
            "Epoch 248/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0656 - val_loss: 0.0901\n",
            "Epoch 249/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0655 - val_loss: 0.0840\n",
            "Epoch 250/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0655 - val_loss: 0.0875\n",
            "Epoch 251/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0655 - val_loss: 0.0699\n",
            "Epoch 252/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0660 - val_loss: 0.0737\n",
            "Epoch 253/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0653 - val_loss: 0.0854\n",
            "Epoch 254/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0704\n",
            "Epoch 255/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0651 - val_loss: 0.0730\n",
            "Epoch 256/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0650 - val_loss: 0.0715\n",
            "Epoch 257/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0652 - val_loss: 0.0741\n",
            "Epoch 258/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0649 - val_loss: 0.0830\n",
            "Epoch 259/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0649 - val_loss: 0.0806\n",
            "Epoch 260/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0649 - val_loss: 0.0708\n",
            "Epoch 261/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0647 - val_loss: 0.0709\n",
            "Epoch 262/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0645 - val_loss: 0.0900\n",
            "Epoch 263/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0662 - val_loss: 0.0858\n",
            "Epoch 264/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0654 - val_loss: 0.0654\n",
            "Epoch 265/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0647 - val_loss: 0.0668\n",
            "Epoch 266/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0648 - val_loss: 0.0662\n",
            "Epoch 267/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0649 - val_loss: 0.0669\n",
            "Epoch 268/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0648 - val_loss: 0.0661\n",
            "Epoch 269/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0649 - val_loss: 0.0855\n",
            "Epoch 270/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0647 - val_loss: 0.0713\n",
            "Epoch 271/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0652 - val_loss: 0.0680\n",
            "Epoch 272/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0934\n",
            "Epoch 273/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0647 - val_loss: 0.0680\n",
            "Epoch 274/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0668\n",
            "Epoch 275/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0816\n",
            "Epoch 276/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0648 - val_loss: 0.0660\n",
            "Epoch 277/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0646 - val_loss: 0.0687\n",
            "Epoch 278/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0645 - val_loss: 0.0764\n",
            "Epoch 279/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0685\n",
            "Epoch 280/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0652 - val_loss: 0.0785\n",
            "Epoch 281/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0650 - val_loss: 0.0720\n",
            "Epoch 282/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0647 - val_loss: 0.0687\n",
            "Epoch 283/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0644 - val_loss: 0.0746\n",
            "Epoch 284/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0649 - val_loss: 0.0687\n",
            "Epoch 285/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0656 - val_loss: 0.0965\n",
            "Epoch 286/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0656 - val_loss: 0.1322\n",
            "Epoch 287/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0649 - val_loss: 0.0838\n",
            "Epoch 288/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0645 - val_loss: 0.0726\n",
            "Epoch 289/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0644 - val_loss: 0.0773\n",
            "Epoch 290/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0647 - val_loss: 0.0747\n",
            "Epoch 291/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0645 - val_loss: 0.0707\n",
            "Epoch 292/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0651 - val_loss: 0.0679\n",
            "Epoch 293/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0648 - val_loss: 0.0874\n",
            "Epoch 294/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0645 - val_loss: 0.0710\n",
            "Epoch 295/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0640 - val_loss: 0.0696\n",
            "Epoch 296/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0660\n",
            "Epoch 297/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0644 - val_loss: 0.0678\n",
            "Epoch 298/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0641 - val_loss: 0.0793\n",
            "Epoch 299/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0641 - val_loss: 0.0733\n",
            "Epoch 300/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0645 - val_loss: 0.0733\n",
            "Epoch 301/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0641 - val_loss: 0.0717\n",
            "Epoch 302/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0644 - val_loss: 0.0885\n",
            "Epoch 303/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0640 - val_loss: 0.0652\n",
            "Epoch 304/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0642 - val_loss: 0.0732\n",
            "Epoch 305/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0641 - val_loss: 0.0661\n",
            "Epoch 306/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0639 - val_loss: 0.0702\n",
            "Epoch 307/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0640 - val_loss: 0.0681\n",
            "Epoch 308/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0645 - val_loss: 0.0762\n",
            "Epoch 309/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 310/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0681\n",
            "Epoch 311/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0668\n",
            "Epoch 312/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0639 - val_loss: 0.0679\n",
            "Epoch 313/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0639 - val_loss: 0.0697\n",
            "Epoch 314/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0644 - val_loss: 0.0885\n",
            "Epoch 315/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0646 - val_loss: 0.0893\n",
            "Epoch 316/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0639 - val_loss: 0.0680\n",
            "Epoch 317/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0735\n",
            "Epoch 318/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0643 - val_loss: 0.0757\n",
            "Epoch 319/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0637 - val_loss: 0.0690\n",
            "Epoch 320/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0636 - val_loss: 0.0714\n",
            "Epoch 321/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0639 - val_loss: 0.0659\n",
            "Epoch 322/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0636 - val_loss: 0.0663\n",
            "Epoch 323/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0638 - val_loss: 0.0662\n",
            "Epoch 324/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0638 - val_loss: 0.0655\n",
            "Epoch 325/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0640 - val_loss: 0.0698\n",
            "Epoch 326/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0737\n",
            "Epoch 327/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0758\n",
            "Epoch 328/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0640 - val_loss: 0.0676\n",
            "Epoch 329/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0639 - val_loss: 0.0683\n",
            "Epoch 330/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0641 - val_loss: 0.0798\n",
            "Epoch 331/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0654\n",
            "Epoch 332/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0638 - val_loss: 0.0740\n",
            "Epoch 333/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0660\n",
            "Epoch 334/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0636 - val_loss: 0.0652\n",
            "Epoch 335/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0668\n",
            "Epoch 336/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0636 - val_loss: 0.0680\n",
            "Epoch 337/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0636 - val_loss: 0.0684\n",
            "Epoch 338/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0638 - val_loss: 0.0662\n",
            "Epoch 339/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0644\n",
            "Epoch 340/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0896\n",
            "Epoch 341/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0636 - val_loss: 0.0797\n",
            "Epoch 342/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0684\n",
            "Epoch 343/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0686\n",
            "Epoch 344/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0698\n",
            "Epoch 345/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0788\n",
            "Epoch 346/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0636 - val_loss: 0.0688\n",
            "Epoch 347/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0658\n",
            "Epoch 348/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0636\n",
            "Epoch 349/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0632 - val_loss: 0.0658\n",
            "Epoch 350/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0631\n",
            "Epoch 351/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0631 - val_loss: 0.0861\n",
            "Epoch 352/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0653\n",
            "Epoch 353/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0631 - val_loss: 0.0659\n",
            "Epoch 354/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0638 - val_loss: 0.0710\n",
            "Epoch 355/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0641 - val_loss: 0.0676\n",
            "Epoch 356/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0735\n",
            "Epoch 357/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0728\n",
            "Epoch 358/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0635 - val_loss: 0.0758\n",
            "Epoch 359/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0700\n",
            "Epoch 360/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0641\n",
            "Epoch 361/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0639 - val_loss: 0.0669\n",
            "Epoch 362/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0633 - val_loss: 0.0710\n",
            "Epoch 363/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0633 - val_loss: 0.0645\n",
            "Epoch 364/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0803\n",
            "Epoch 365/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0642 - val_loss: 0.0887\n",
            "Epoch 366/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0643 - val_loss: 0.0687\n",
            "Epoch 367/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0631 - val_loss: 0.0677\n",
            "Epoch 368/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0632 - val_loss: 0.0680\n",
            "Epoch 369/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0632\n",
            "Epoch 370/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0642\n",
            "Epoch 371/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0708\n",
            "Epoch 372/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0706\n",
            "Epoch 373/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0724\n",
            "Epoch 374/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0632 - val_loss: 0.0746\n",
            "Epoch 375/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0710\n",
            "Epoch 376/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0631 - val_loss: 0.0712\n",
            "Epoch 377/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0638 - val_loss: 0.0672\n",
            "Epoch 378/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0631 - val_loss: 0.0759\n",
            "Epoch 379/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0654\n",
            "Epoch 380/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0664\n",
            "Epoch 381/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0731\n",
            "Epoch 382/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0724\n",
            "Epoch 383/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0639\n",
            "Epoch 384/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0775\n",
            "Epoch 385/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0631 - val_loss: 0.0736\n",
            "Epoch 386/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0728\n",
            "Epoch 387/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0640 - val_loss: 0.0823\n",
            "Epoch 388/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0637 - val_loss: 0.0812\n",
            "Epoch 389/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0646\n",
            "Epoch 390/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0632 - val_loss: 0.0712\n",
            "Epoch 391/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0698\n",
            "Epoch 392/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0664\n",
            "Epoch 393/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0684\n",
            "Epoch 394/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0737\n",
            "Epoch 395/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0651\n",
            "Epoch 396/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0669\n",
            "Epoch 397/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0641\n",
            "Epoch 398/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0702\n",
            "Epoch 399/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0725\n",
            "Epoch 400/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0683\n",
            "Epoch 401/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0627 - val_loss: 0.0672\n",
            "Epoch 402/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0772\n",
            "Epoch 403/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0685\n",
            "Epoch 404/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0629 - val_loss: 0.0739\n",
            "Epoch 405/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0741\n",
            "Epoch 406/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0624 - val_loss: 0.0680\n",
            "Epoch 407/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0749\n",
            "Epoch 408/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0634 - val_loss: 0.0776\n",
            "Epoch 409/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0631 - val_loss: 0.0759\n",
            "Epoch 410/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0625 - val_loss: 0.0699\n",
            "Epoch 411/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0621 - val_loss: 0.0630\n",
            "Epoch 412/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0627 - val_loss: 0.0702\n",
            "Epoch 413/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0630 - val_loss: 0.0724\n",
            "Epoch 414/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0625 - val_loss: 0.0687\n",
            "Epoch 415/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0647\n",
            "Epoch 416/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0680\n",
            "Epoch 417/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0669\n",
            "Epoch 418/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0668\n",
            "Epoch 419/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0655\n",
            "Epoch 420/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0635\n",
            "Epoch 421/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0641\n",
            "Epoch 422/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0654\n",
            "Epoch 423/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0802\n",
            "Epoch 424/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0693\n",
            "Epoch 425/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0635\n",
            "Epoch 426/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0623 - val_loss: 0.0722\n",
            "Epoch 427/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0621 - val_loss: 0.0666\n",
            "Epoch 428/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0661\n",
            "Epoch 429/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0678\n",
            "Epoch 430/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0651\n",
            "Epoch 431/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0625 - val_loss: 0.0657\n",
            "Epoch 432/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0737\n",
            "Epoch 433/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0650\n",
            "Epoch 434/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0625 - val_loss: 0.0650\n",
            "Epoch 435/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0910\n",
            "Epoch 436/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0691\n",
            "Epoch 437/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0621 - val_loss: 0.0716\n",
            "Epoch 438/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0741\n",
            "Epoch 439/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0668\n",
            "Epoch 440/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0663\n",
            "Epoch 441/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0694\n",
            "Epoch 442/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0627 - val_loss: 0.0754\n",
            "Epoch 443/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0629 - val_loss: 0.0750\n",
            "Epoch 444/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0733\n",
            "Epoch 445/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0709\n",
            "Epoch 446/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0621 - val_loss: 0.0750\n",
            "Epoch 447/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0643\n",
            "Epoch 448/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0621 - val_loss: 0.0756\n",
            "Epoch 449/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0663\n",
            "Epoch 450/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0626\n",
            "Epoch 451/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0622 - val_loss: 0.0666\n",
            "Epoch 452/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0617 - val_loss: 0.0729\n",
            "Epoch 453/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0620 - val_loss: 0.0631\n",
            "Epoch 454/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0661\n",
            "Epoch 455/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0624 - val_loss: 0.0641\n",
            "Epoch 456/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0639\n",
            "Epoch 457/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0619 - val_loss: 0.0637\n",
            "Epoch 458/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0620 - val_loss: 0.0782\n",
            "Epoch 459/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0678\n",
            "Epoch 460/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0684\n",
            "Epoch 461/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0748\n",
            "Epoch 462/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0632\n",
            "Epoch 463/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0667\n",
            "Epoch 464/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0694\n",
            "Epoch 465/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0693\n",
            "Epoch 466/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0622 - val_loss: 0.0710\n",
            "Epoch 467/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0644\n",
            "Epoch 468/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0744\n",
            "Epoch 469/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0621 - val_loss: 0.0654\n",
            "Epoch 470/500\n",
            "4480/4480 [==============================] - 17s 4ms/sample - loss: 0.0619 - val_loss: 0.0646\n",
            "Epoch 471/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0719\n",
            "Epoch 472/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0625 - val_loss: 0.0744\n",
            "Epoch 473/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0720\n",
            "Epoch 474/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0716\n",
            "Epoch 475/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0662\n",
            "Epoch 476/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0623 - val_loss: 0.0635\n",
            "Epoch 477/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0618 - val_loss: 0.0634\n",
            "Epoch 478/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0651\n",
            "Epoch 479/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0649\n",
            "Epoch 480/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0617 - val_loss: 0.0677\n",
            "Epoch 481/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0615 - val_loss: 0.0709\n",
            "Epoch 482/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0617 - val_loss: 0.0681\n",
            "Epoch 483/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0626 - val_loss: 0.0776\n",
            "Epoch 484/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0698\n",
            "Epoch 485/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0615 - val_loss: 0.0617\n",
            "Epoch 486/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0613 - val_loss: 0.0642\n",
            "Epoch 487/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0615 - val_loss: 0.0624\n",
            "Epoch 488/500\n",
            "4480/4480 [==============================] - 19s 4ms/sample - loss: 0.0614 - val_loss: 0.0683\n",
            "Epoch 489/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0615 - val_loss: 0.0724\n",
            "Epoch 490/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0616 - val_loss: 0.0686\n",
            "Epoch 491/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0616 - val_loss: 0.0628\n",
            "Epoch 492/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0620 - val_loss: 0.0685\n",
            "Epoch 493/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0620 - val_loss: 0.0655\n",
            "Epoch 494/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0625 - val_loss: 0.0781\n",
            "Epoch 495/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0619 - val_loss: 0.0644\n",
            "Epoch 496/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0617 - val_loss: 0.0665\n",
            "Epoch 497/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0616 - val_loss: 0.0675\n",
            "Epoch 498/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0615 - val_loss: 0.0688\n",
            "Epoch 499/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0614 - val_loss: 0.0640\n",
            "Epoch 500/500\n",
            "4480/4480 [==============================] - 18s 4ms/sample - loss: 0.0612 - val_loss: 0.0658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx4-kyYbGfI",
        "colab_type": "text"
      },
      "source": [
        "## Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1iTz86KbNed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "da24cc4e-76a5-41e5-9a1e-c49c76fcf52f"
      },
      "source": [
        "model.save_weights(os.path.join(os.getcwd(),'core','model_weights','manNet.h5'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1M9sjHIbSnf",
        "colab_type": "text"
      },
      "source": [
        "## Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNSS7XIbXaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "1361fc38-374c-4fc0-973a-47caf9674ac6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fcde0178080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
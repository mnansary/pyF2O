{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_gen_manNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_manNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojVYZ7Spzpv",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   Check TF version\n",
        "*    TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q4JaV2ps6z",
        "colab_type": "code",
        "outputId": "a847f1be-091c-48e9-eaee-73408bdaa6ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVhQoAOqGGW",
        "colab_type": "code",
        "outputId": "31b7a0e3-0e9d-47dc-a862-243845ba2b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/F2O/pyF2O"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/F2O/pyF2O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vp4y0TiqKJh",
        "colab_type": "code",
        "outputId": "c431eb44-ec24-4801-8e4d-a63fa83776cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 43.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.2.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692-uM-yqdaF",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9QkSiJqed6",
        "colab_type": "code",
        "outputId": "5a7612b8-ca21-4704-9488-21cf83bdeee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.120.34.242:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 6884043169621902477),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10786115097073925511),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1411116487564481424),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9835207646975880239),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7931623725910259319),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15673732517948685516),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2362699927920710070),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6753826123422703955),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9261186044788426654),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12746404476816881959),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14461160174219374452)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefiHZ4qlHA",
        "colab_type": "text"
      },
      "source": [
        "# manNet generator Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwtSzOarVYM",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOro7D1krWYf",
        "colab_type": "code",
        "outputId": "4d844df4-c2c7-4a40-c730-e77749f1c1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import mean_absolute_error\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from core.generators import man_net\n",
        "model=man_net()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(lr=0.01), loss=mean_absolute_error)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 8)      4104        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 8)      36872       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 8)      102408      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 8, 24)     0           conv2d_10[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 8, 8, 24)     96          concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 8, 8, 24)     0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 16, 16, 24)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 6)    150         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 6)    1302        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 6)    3606        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 16, 16, 18)   0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 16, 16, 18)   72          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 16, 16, 18)   0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 32, 32, 18)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 4)    76          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 4)    652         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 4)    1804        lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 32, 32, 12)   48          concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 12)   0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 64, 64, 12)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 64, 64, 2)    26          lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 64, 64, 2)    218         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 64, 64, 2)    602         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 64, 64, 6)    0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 64, 64, 6)    24          concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 6)    0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 128, 128, 6)  0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 128, 128, 2)  302         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 128, 128, 2)  590         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 128, 128, 2)  1454        lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 128, 128, 6)  0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 128, 128, 6)  24          concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 128, 128, 6)  0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 128, 128, 3)  165         activation_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 7,789,859\n",
            "Trainable params: 7,789,727\n",
            "Non-trainable params: 132\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fqLlsIcrgJ6",
        "colab_type": "text"
      },
      "source": [
        "## Convert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLuMCkq5rha3",
        "colab_type": "code",
        "outputId": "940f5aea-5280-458d-faef-dc8a70250e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def convert_model_TPU(model):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "                                           strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                                               tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "                                               )\n",
        "                                           )\n",
        "\n",
        "model=convert_model_TPU(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.120.34.242:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6884043169621902477)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10786115097073925511)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1411116487564481424)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9835207646975880239)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7931623725910259319)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15673732517948685516)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2362699927920710070)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6753826123422703955)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9261186044788426654)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12746404476816881959)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14461160174219374452)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMWNlaqLNIPY",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "* Load **X_Train_ALL.h5** and **Y_Train_ALL.h5**\n",
        "* Normalize both dataset as float32\n",
        "* Split the data for **validation** and **training**\n",
        "* Drop Batch Reminders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WMdSlCZJPWi",
        "colab_type": "code",
        "outputId": "a23eb0cf-e372-49bc-b09a-023f2b437813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from core.utils import readh5\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "class TRAIN_FLAGS:\n",
        "    H5_DIR   = '/content/gdrive/My Drive/PROJECTS/F2O/H5Data/' # @param\n",
        "    BATCH_SIZE      = 128  # @param\n",
        "    NUM_EPOCHS      = 200  # @param\n",
        "    IMAGE_DIM       = 128  # @param\n",
        "    X_IDEN          = 'X_Train_ALL.h5'  # @param\n",
        "    Y_IDEN          = 'Y_Train_ALL.h5'  # @param\n",
        "\n",
        "X=readh5(os.path.join(TRAIN_FLAGS.H5_DIR,TRAIN_FLAGS.X_IDEN))\n",
        "Y=readh5(os.path.join(TRAIN_FLAGS.H5_DIR,TRAIN_FLAGS.Y_IDEN))\n",
        "# Normalize\n",
        "X=X.astype('float32')/255.0\n",
        "Y=Y.astype('float32')/255.0\n",
        "# Test And Validation\n",
        "Xt,Xv,Yt,Yv=train_test_split(X,Y,test_size=0.2)\n",
        "\n",
        "print(\"Before Drop\")\n",
        "print(\"X_train: {}\".format(Xt.shape))\n",
        "print(\"Y_train: {}\".format(Yt.shape))\n",
        "print(\"X_eval: {}\".format(Xv.shape))\n",
        "print(\"Y_eval: {}\".format(Yv.shape))\n",
        "\n",
        "# Drop Batch Residue\n",
        "nb_sig_train=int(Xt.shape[0]/TRAIN_FLAGS.BATCH_SIZE)*TRAIN_FLAGS.BATCH_SIZE \n",
        "nb_sig_eval=int(Xv.shape[0]/TRAIN_FLAGS.BATCH_SIZE)*TRAIN_FLAGS.BATCH_SIZE \n",
        "\n",
        "Xt=Xt[:nb_sig_train,:,:,:]\n",
        "Yt=Yt[:nb_sig_train,:,:,:]\n",
        "Xv=Xv[:nb_sig_eval,:,:,:]\n",
        "Yv=Yv[:nb_sig_eval,:,:,:]\n",
        "\n",
        "print(\"After Drop\")\n",
        "print(\"X_train: {}\".format(Xt.shape))\n",
        "print(\"Y_train: {}\".format(Yt.shape))\n",
        "print(\"X_eval: {}\".format(Xv.shape))\n",
        "print(\"Y_eval: {}\".format(Yv.shape))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Drop\n",
            "X_train: (4480, 128, 128, 3)\n",
            "Y_train: (4480, 128, 128, 3)\n",
            "X_eval: (1120, 128, 128, 3)\n",
            "Y_eval: (1120, 128, 128, 3)\n",
            "After Drop\n",
            "X_train: (4480, 128, 128, 3)\n",
            "Y_train: (4480, 128, 128, 3)\n",
            "X_eval: (1024, 128, 128, 3)\n",
            "Y_eval: (1024, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQXx8YRNSyr",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTUQlweNYYN",
        "colab_type": "code",
        "outputId": "6727a8e9-c5da-4dbf-b537-8a6c1d65d285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "\n",
        "c_path=os.path.join(os.getcwd(),'core','model_weights')\n",
        "print(c_path)\n",
        "model_name='manNet'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(c_path,'{}.h5'.format(model_name)), verbose=1, save_best_only=True)\n",
        "\n",
        "history=model.fit(Xt,Yt,validation_data=(Xv,Yv),callbacks=[checkpoint],epochs=TRAIN_FLAGS.NUM_EPOCHS,batch_size=TRAIN_FLAGS.BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights\n",
            "Train on 4480 samples, validate on 1024 samples\n",
            "Epoch 1/200\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7efe8e1a9518> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 46.67950701713562 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.009999999776482582 {0.01}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4352/4480 [============================>.] - ETA: 2s - loss: 0.1685INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7efe83952d30> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.033447265625 secs\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.47297, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 138s 31ms/sample - loss: 0.1672 - val_loss: 0.4730\n",
            "Epoch 2/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1246\n",
            "Epoch 00002: val_loss did not improve from 0.47297\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.1244 - val_loss: 0.4837\n",
            "Epoch 3/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1139\n",
            "Epoch 00003: val_loss improved from 0.47297 to 0.18812, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 27s 6ms/sample - loss: 0.1138 - val_loss: 0.1881\n",
            "Epoch 4/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1104\n",
            "Epoch 00004: val_loss improved from 0.18812 to 0.17225, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.1104 - val_loss: 0.1722\n",
            "Epoch 5/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1078\n",
            "Epoch 00005: val_loss improved from 0.17225 to 0.13137, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 27s 6ms/sample - loss: 0.1077 - val_loss: 0.1314\n",
            "Epoch 6/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1054\n",
            "Epoch 00006: val_loss did not improve from 0.13137\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.1055 - val_loss: 0.3238\n",
            "Epoch 7/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1037\n",
            "Epoch 00007: val_loss did not improve from 0.13137\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.1035 - val_loss: 0.2466\n",
            "Epoch 8/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1022\n",
            "Epoch 00008: val_loss did not improve from 0.13137\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.1022 - val_loss: 0.2234\n",
            "Epoch 9/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1014\n",
            "Epoch 00009: val_loss improved from 0.13137 to 0.13108, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 27s 6ms/sample - loss: 0.1013 - val_loss: 0.1311\n",
            "Epoch 10/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.1002\n",
            "Epoch 00010: val_loss improved from 0.13108 to 0.12417, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.1000 - val_loss: 0.1242\n",
            "Epoch 11/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0981\n",
            "Epoch 00011: val_loss improved from 0.12417 to 0.11922, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0981 - val_loss: 0.1192\n",
            "Epoch 12/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0974\n",
            "Epoch 00012: val_loss improved from 0.11922 to 0.11060, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 27s 6ms/sample - loss: 0.0975 - val_loss: 0.1106\n",
            "Epoch 13/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0968\n",
            "Epoch 00013: val_loss did not improve from 0.11060\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0968 - val_loss: 0.1133\n",
            "Epoch 14/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0954\n",
            "Epoch 00014: val_loss did not improve from 0.11060\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0955 - val_loss: 0.1199\n",
            "Epoch 15/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0947\n",
            "Epoch 00015: val_loss did not improve from 0.11060\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0947 - val_loss: 0.1137\n",
            "Epoch 16/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0946\n",
            "Epoch 00016: val_loss improved from 0.11060 to 0.10654, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0946 - val_loss: 0.1065\n",
            "Epoch 17/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0935\n",
            "Epoch 00017: val_loss did not improve from 0.10654\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0935 - val_loss: 0.1241\n",
            "Epoch 18/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0927\n",
            "Epoch 00018: val_loss did not improve from 0.10654\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0928 - val_loss: 0.1131\n",
            "Epoch 19/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0925\n",
            "Epoch 00019: val_loss improved from 0.10654 to 0.09921, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0924 - val_loss: 0.0992\n",
            "Epoch 20/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0912\n",
            "Epoch 00020: val_loss did not improve from 0.09921\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0912 - val_loss: 0.1179\n",
            "Epoch 21/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0905\n",
            "Epoch 00021: val_loss improved from 0.09921 to 0.09715, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0904 - val_loss: 0.0972\n",
            "Epoch 22/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0898\n",
            "Epoch 00022: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0899 - val_loss: 0.1289\n",
            "Epoch 23/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0888\n",
            "Epoch 00023: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0888 - val_loss: 0.0978\n",
            "Epoch 24/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0890\n",
            "Epoch 00024: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0889 - val_loss: 0.1013\n",
            "Epoch 25/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0886\n",
            "Epoch 00025: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0886 - val_loss: 0.1336\n",
            "Epoch 26/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0874\n",
            "Epoch 00026: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0874 - val_loss: 0.1358\n",
            "Epoch 27/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0868\n",
            "Epoch 00027: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0869 - val_loss: 0.1054\n",
            "Epoch 28/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0868\n",
            "Epoch 00028: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0868 - val_loss: 0.1574\n",
            "Epoch 29/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0856\n",
            "Epoch 00029: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0854 - val_loss: 0.1071\n",
            "Epoch 30/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0856\n",
            "Epoch 00030: val_loss did not improve from 0.09715\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0858 - val_loss: 0.1119\n",
            "Epoch 31/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0853\n",
            "Epoch 00031: val_loss improved from 0.09715 to 0.08466, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0853 - val_loss: 0.0847\n",
            "Epoch 32/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0845\n",
            "Epoch 00032: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0847 - val_loss: 0.0936\n",
            "Epoch 33/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0846\n",
            "Epoch 00033: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0847 - val_loss: 0.0904\n",
            "Epoch 34/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0841\n",
            "Epoch 00034: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0841 - val_loss: 0.0865\n",
            "Epoch 35/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0844\n",
            "Epoch 00035: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0842 - val_loss: 0.1133\n",
            "Epoch 36/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0834\n",
            "Epoch 00036: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0834 - val_loss: 0.0849\n",
            "Epoch 37/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0831\n",
            "Epoch 00037: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0832 - val_loss: 0.1005\n",
            "Epoch 38/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0825\n",
            "Epoch 00038: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0824 - val_loss: 0.0861\n",
            "Epoch 39/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0828\n",
            "Epoch 00039: val_loss did not improve from 0.08466\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0827 - val_loss: 0.1124\n",
            "Epoch 40/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0822\n",
            "Epoch 00040: val_loss improved from 0.08466 to 0.08155, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0822 - val_loss: 0.0816\n",
            "Epoch 41/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0821\n",
            "Epoch 00041: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0820 - val_loss: 0.0870\n",
            "Epoch 42/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0826\n",
            "Epoch 00042: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0826 - val_loss: 0.2619\n",
            "Epoch 43/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0815\n",
            "Epoch 00043: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0815 - val_loss: 0.0896\n",
            "Epoch 44/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0821\n",
            "Epoch 00044: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0820 - val_loss: 0.1961\n",
            "Epoch 45/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0807\n",
            "Epoch 00045: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0808 - val_loss: 0.1124\n",
            "Epoch 46/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0811\n",
            "Epoch 00046: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0811 - val_loss: 0.0976\n",
            "Epoch 47/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0804\n",
            "Epoch 00047: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0803 - val_loss: 0.0944\n",
            "Epoch 48/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0802\n",
            "Epoch 00048: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0803 - val_loss: 0.1296\n",
            "Epoch 49/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0801\n",
            "Epoch 00049: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0801 - val_loss: 0.1347\n",
            "Epoch 50/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0795\n",
            "Epoch 00050: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0795 - val_loss: 0.0872\n",
            "Epoch 51/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0797\n",
            "Epoch 00051: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0796 - val_loss: 0.0963\n",
            "Epoch 52/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0787\n",
            "Epoch 00052: val_loss did not improve from 0.08155\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0787 - val_loss: 0.0872\n",
            "Epoch 53/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0784\n",
            "Epoch 00053: val_loss improved from 0.08155 to 0.08123, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 27s 6ms/sample - loss: 0.0785 - val_loss: 0.0812\n",
            "Epoch 54/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0782\n",
            "Epoch 00054: val_loss did not improve from 0.08123\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0782 - val_loss: 0.0877\n",
            "Epoch 55/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0779\n",
            "Epoch 00055: val_loss improved from 0.08123 to 0.07692, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 27s 6ms/sample - loss: 0.0778 - val_loss: 0.0769\n",
            "Epoch 56/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0783\n",
            "Epoch 00056: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0782 - val_loss: 0.0786\n",
            "Epoch 57/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0770\n",
            "Epoch 00057: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0770 - val_loss: 0.0808\n",
            "Epoch 58/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0776\n",
            "Epoch 00058: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0775 - val_loss: 0.0820\n",
            "Epoch 59/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0774\n",
            "Epoch 00059: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0776 - val_loss: 0.0984\n",
            "Epoch 60/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0782\n",
            "Epoch 00060: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0782 - val_loss: 0.0826\n",
            "Epoch 61/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0768\n",
            "Epoch 00061: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0767 - val_loss: 0.0798\n",
            "Epoch 62/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0775\n",
            "Epoch 00062: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0774 - val_loss: 0.0826\n",
            "Epoch 63/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0775\n",
            "Epoch 00063: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0774 - val_loss: 0.0804\n",
            "Epoch 64/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0772\n",
            "Epoch 00064: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0773 - val_loss: 0.1064\n",
            "Epoch 65/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0766\n",
            "Epoch 00065: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0766 - val_loss: 0.0986\n",
            "Epoch 66/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0762\n",
            "Epoch 00066: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0762 - val_loss: 0.0799\n",
            "Epoch 67/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0764\n",
            "Epoch 00067: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0764 - val_loss: 0.0986\n",
            "Epoch 68/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0761\n",
            "Epoch 00068: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0760 - val_loss: 0.1164\n",
            "Epoch 69/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0759\n",
            "Epoch 00069: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0759 - val_loss: 0.0928\n",
            "Epoch 70/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0758\n",
            "Epoch 00070: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0757 - val_loss: 0.1003\n",
            "Epoch 71/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0759\n",
            "Epoch 00071: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0758 - val_loss: 0.1477\n",
            "Epoch 72/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0759\n",
            "Epoch 00072: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0760 - val_loss: 0.1092\n",
            "Epoch 73/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0756\n",
            "Epoch 00073: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0756 - val_loss: 0.0907\n",
            "Epoch 74/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0752\n",
            "Epoch 00074: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0752 - val_loss: 0.0926\n",
            "Epoch 75/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0753\n",
            "Epoch 00075: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0752 - val_loss: 0.1113\n",
            "Epoch 76/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0748\n",
            "Epoch 00076: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0748 - val_loss: 0.1112\n",
            "Epoch 77/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0742\n",
            "Epoch 00077: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0742 - val_loss: 0.0793\n",
            "Epoch 78/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0738\n",
            "Epoch 00078: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0739 - val_loss: 0.0928\n",
            "Epoch 79/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0746\n",
            "Epoch 00079: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0745 - val_loss: 0.1047\n",
            "Epoch 80/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0744\n",
            "Epoch 00080: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0743 - val_loss: 0.0899\n",
            "Epoch 81/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0743\n",
            "Epoch 00081: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0742 - val_loss: 0.0901\n",
            "Epoch 82/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0735\n",
            "Epoch 00082: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0736 - val_loss: 0.0807\n",
            "Epoch 83/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0739\n",
            "Epoch 00083: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0740 - val_loss: 0.0971\n",
            "Epoch 84/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0731\n",
            "Epoch 00084: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0731 - val_loss: 0.0787\n",
            "Epoch 85/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0739\n",
            "Epoch 00085: val_loss did not improve from 0.07692\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0740 - val_loss: 0.1322\n",
            "Epoch 86/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0740\n",
            "Epoch 00086: val_loss improved from 0.07692 to 0.07379, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0740 - val_loss: 0.0738\n",
            "Epoch 87/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0731\n",
            "Epoch 00087: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0730 - val_loss: 0.0892\n",
            "Epoch 88/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0731\n",
            "Epoch 00088: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0731 - val_loss: 0.0767\n",
            "Epoch 89/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0727\n",
            "Epoch 00089: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0727 - val_loss: 0.0754\n",
            "Epoch 90/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0733\n",
            "Epoch 00090: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0733 - val_loss: 0.0926\n",
            "Epoch 91/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0731\n",
            "Epoch 00091: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0731 - val_loss: 0.0939\n",
            "Epoch 92/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0734\n",
            "Epoch 00092: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0733 - val_loss: 0.0776\n",
            "Epoch 93/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0732\n",
            "Epoch 00093: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0732 - val_loss: 0.1137\n",
            "Epoch 94/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0726\n",
            "Epoch 00094: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0726 - val_loss: 0.0740\n",
            "Epoch 95/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0722\n",
            "Epoch 00095: val_loss did not improve from 0.07379\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0722 - val_loss: 0.0778\n",
            "Epoch 96/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0724\n",
            "Epoch 00096: val_loss improved from 0.07379 to 0.07227, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0724 - val_loss: 0.0723\n",
            "Epoch 97/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0721\n",
            "Epoch 00097: val_loss did not improve from 0.07227\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0722 - val_loss: 0.0745\n",
            "Epoch 98/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0725\n",
            "Epoch 00098: val_loss did not improve from 0.07227\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0726 - val_loss: 0.1012\n",
            "Epoch 99/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0726\n",
            "Epoch 00099: val_loss did not improve from 0.07227\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0725 - val_loss: 0.1233\n",
            "Epoch 100/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0730\n",
            "Epoch 00100: val_loss did not improve from 0.07227\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0729 - val_loss: 0.0935\n",
            "Epoch 101/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0712\n",
            "Epoch 00101: val_loss improved from 0.07227 to 0.07218, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0713 - val_loss: 0.0722\n",
            "Epoch 102/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0717\n",
            "Epoch 00102: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0716 - val_loss: 0.0916\n",
            "Epoch 103/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0725\n",
            "Epoch 00103: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0724 - val_loss: 0.0899\n",
            "Epoch 104/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0717\n",
            "Epoch 00104: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0716 - val_loss: 0.0771\n",
            "Epoch 105/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0709\n",
            "Epoch 00105: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0709 - val_loss: 0.0745\n",
            "Epoch 106/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0710\n",
            "Epoch 00106: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0710 - val_loss: 0.0758\n",
            "Epoch 107/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0714\n",
            "Epoch 00107: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0715 - val_loss: 0.0848\n",
            "Epoch 108/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0715\n",
            "Epoch 00108: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0715 - val_loss: 0.0781\n",
            "Epoch 109/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0714\n",
            "Epoch 00109: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0713 - val_loss: 0.0937\n",
            "Epoch 110/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0710\n",
            "Epoch 00110: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0710 - val_loss: 0.0818\n",
            "Epoch 111/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0713\n",
            "Epoch 00111: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0712 - val_loss: 0.0781\n",
            "Epoch 112/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0711\n",
            "Epoch 00112: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0711 - val_loss: 0.0897\n",
            "Epoch 113/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0715\n",
            "Epoch 00113: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0715 - val_loss: 0.0976\n",
            "Epoch 114/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0714\n",
            "Epoch 00114: val_loss did not improve from 0.07218\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0715 - val_loss: 0.0834\n",
            "Epoch 115/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0704\n",
            "Epoch 00115: val_loss improved from 0.07218 to 0.07004, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0705 - val_loss: 0.0700\n",
            "Epoch 116/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0704\n",
            "Epoch 00116: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0704 - val_loss: 0.0865\n",
            "Epoch 117/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0702\n",
            "Epoch 00117: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0702 - val_loss: 0.0734\n",
            "Epoch 118/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0705\n",
            "Epoch 00118: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0706 - val_loss: 0.0770\n",
            "Epoch 119/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0709\n",
            "Epoch 00119: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0708 - val_loss: 0.0850\n",
            "Epoch 120/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0704\n",
            "Epoch 00120: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0704 - val_loss: 0.0827\n",
            "Epoch 121/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0701\n",
            "Epoch 00121: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0702 - val_loss: 0.0756\n",
            "Epoch 122/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0705\n",
            "Epoch 00122: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0704 - val_loss: 0.0791\n",
            "Epoch 123/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0701\n",
            "Epoch 00123: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0701 - val_loss: 0.0847\n",
            "Epoch 124/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0698\n",
            "Epoch 00124: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0699 - val_loss: 0.0839\n",
            "Epoch 125/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0693\n",
            "Epoch 00125: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0695 - val_loss: 0.0750\n",
            "Epoch 126/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0707\n",
            "Epoch 00126: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0707 - val_loss: 0.1040\n",
            "Epoch 127/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0698\n",
            "Epoch 00127: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0699 - val_loss: 0.0759\n",
            "Epoch 128/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0700\n",
            "Epoch 00128: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0699 - val_loss: 0.0964\n",
            "Epoch 129/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0696\n",
            "Epoch 00129: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0696 - val_loss: 0.0803\n",
            "Epoch 130/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0698\n",
            "Epoch 00130: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0697 - val_loss: 0.0842\n",
            "Epoch 131/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0697\n",
            "Epoch 00131: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0697 - val_loss: 0.0767\n",
            "Epoch 132/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0700\n",
            "Epoch 00132: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0700 - val_loss: 0.1009\n",
            "Epoch 133/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0697\n",
            "Epoch 00133: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0696 - val_loss: 0.0720\n",
            "Epoch 134/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0698\n",
            "Epoch 00134: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0698 - val_loss: 0.1023\n",
            "Epoch 135/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0698\n",
            "Epoch 00135: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0698 - val_loss: 0.1033\n",
            "Epoch 136/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0702\n",
            "Epoch 00136: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0701 - val_loss: 0.0828\n",
            "Epoch 137/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0699\n",
            "Epoch 00137: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0699 - val_loss: 0.1025\n",
            "Epoch 138/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0695\n",
            "Epoch 00138: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0694 - val_loss: 0.0863\n",
            "Epoch 139/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0689\n",
            "Epoch 00139: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0689 - val_loss: 0.0813\n",
            "Epoch 140/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0692\n",
            "Epoch 00140: val_loss did not improve from 0.07004\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0692 - val_loss: 0.0713\n",
            "Epoch 141/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0689\n",
            "Epoch 00141: val_loss improved from 0.07004 to 0.06971, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0689 - val_loss: 0.0697\n",
            "Epoch 142/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0685\n",
            "Epoch 00142: val_loss did not improve from 0.06971\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0684 - val_loss: 0.0768\n",
            "Epoch 143/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0691\n",
            "Epoch 00143: val_loss did not improve from 0.06971\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0691 - val_loss: 0.0800\n",
            "Epoch 144/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0692\n",
            "Epoch 00144: val_loss improved from 0.06971 to 0.06936, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0691 - val_loss: 0.0694\n",
            "Epoch 145/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0690\n",
            "Epoch 00145: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0689 - val_loss: 0.0732\n",
            "Epoch 146/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0688\n",
            "Epoch 00146: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0688 - val_loss: 0.0828\n",
            "Epoch 147/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0685\n",
            "Epoch 00147: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0685 - val_loss: 0.0729\n",
            "Epoch 148/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0687\n",
            "Epoch 00148: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0687 - val_loss: 0.0706\n",
            "Epoch 149/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0688\n",
            "Epoch 00149: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0688 - val_loss: 0.1086\n",
            "Epoch 150/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0686\n",
            "Epoch 00150: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0686 - val_loss: 0.0716\n",
            "Epoch 151/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0683\n",
            "Epoch 00151: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0682 - val_loss: 0.0830\n",
            "Epoch 152/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0690\n",
            "Epoch 00152: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0690 - val_loss: 0.0892\n",
            "Epoch 153/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0684\n",
            "Epoch 00153: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0683 - val_loss: 0.0699\n",
            "Epoch 154/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0684\n",
            "Epoch 00154: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0684 - val_loss: 0.0857\n",
            "Epoch 155/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0686\n",
            "Epoch 00155: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0685 - val_loss: 0.0820\n",
            "Epoch 156/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0686\n",
            "Epoch 00156: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0686 - val_loss: 0.0753\n",
            "Epoch 157/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0681\n",
            "Epoch 00157: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0682 - val_loss: 0.0695\n",
            "Epoch 158/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00158: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0679 - val_loss: 0.0718\n",
            "Epoch 159/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0684\n",
            "Epoch 00159: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0684 - val_loss: 0.0815\n",
            "Epoch 160/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0680\n",
            "Epoch 00160: val_loss did not improve from 0.06936\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0681 - val_loss: 0.0751\n",
            "Epoch 161/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0675\n",
            "Epoch 00161: val_loss improved from 0.06936 to 0.06764, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0676 - val_loss: 0.0676\n",
            "Epoch 162/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0681\n",
            "Epoch 00162: val_loss did not improve from 0.06764\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0680 - val_loss: 0.0874\n",
            "Epoch 163/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0682\n",
            "Epoch 00163: val_loss did not improve from 0.06764\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0681 - val_loss: 0.1024\n",
            "Epoch 164/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0675\n",
            "Epoch 00164: val_loss did not improve from 0.06764\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0675 - val_loss: 0.0801\n",
            "Epoch 165/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0675\n",
            "Epoch 00165: val_loss did not improve from 0.06764\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0675 - val_loss: 0.0694\n",
            "Epoch 166/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0673\n",
            "Epoch 00166: val_loss improved from 0.06764 to 0.06756, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0673 - val_loss: 0.0676\n",
            "Epoch 167/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0673\n",
            "Epoch 00167: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0672 - val_loss: 0.0799\n",
            "Epoch 168/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0678\n",
            "Epoch 00168: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0678 - val_loss: 0.0714\n",
            "Epoch 169/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0681\n",
            "Epoch 00169: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0680 - val_loss: 0.0955\n",
            "Epoch 170/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0674\n",
            "Epoch 00170: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0674 - val_loss: 0.0748\n",
            "Epoch 171/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0673\n",
            "Epoch 00171: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0673 - val_loss: 0.0705\n",
            "Epoch 172/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00172: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0678 - val_loss: 0.0820\n",
            "Epoch 173/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0674\n",
            "Epoch 00173: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0674 - val_loss: 0.0704\n",
            "Epoch 174/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00174: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0678 - val_loss: 0.0884\n",
            "Epoch 175/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0684\n",
            "Epoch 00175: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0683 - val_loss: 0.1116\n",
            "Epoch 176/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0679\n",
            "Epoch 00176: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0679 - val_loss: 0.0878\n",
            "Epoch 177/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0679\n",
            "Epoch 00177: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0679 - val_loss: 0.0881\n",
            "Epoch 178/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0670\n",
            "Epoch 00178: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0670 - val_loss: 0.0754\n",
            "Epoch 179/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0670\n",
            "Epoch 00179: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0670 - val_loss: 0.0695\n",
            "Epoch 180/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0666\n",
            "Epoch 00180: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0667 - val_loss: 0.0682\n",
            "Epoch 181/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00181: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 14s 3ms/sample - loss: 0.0668 - val_loss: 0.0790\n",
            "Epoch 182/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0678\n",
            "Epoch 00182: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0678 - val_loss: 0.0744\n",
            "Epoch 183/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0666\n",
            "Epoch 00183: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0666 - val_loss: 0.0787\n",
            "Epoch 184/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0674\n",
            "Epoch 00184: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0674 - val_loss: 0.0888\n",
            "Epoch 185/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00185: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0668 - val_loss: 0.0753\n",
            "Epoch 186/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0669\n",
            "Epoch 00186: val_loss improved from 0.06756 to 0.06756, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0670 - val_loss: 0.0676\n",
            "Epoch 187/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0669\n",
            "Epoch 00187: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0670 - val_loss: 0.0757\n",
            "Epoch 188/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0677\n",
            "Epoch 00188: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0676 - val_loss: 0.0791\n",
            "Epoch 189/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00189: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0668 - val_loss: 0.0790\n",
            "Epoch 190/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00190: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0668 - val_loss: 0.0698\n",
            "Epoch 191/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00191: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0670 - val_loss: 0.0758\n",
            "Epoch 192/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0674\n",
            "Epoch 00192: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0675 - val_loss: 0.0866\n",
            "Epoch 193/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0668\n",
            "Epoch 00193: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0668 - val_loss: 0.0888\n",
            "Epoch 194/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0662\n",
            "Epoch 00194: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0663 - val_loss: 0.0765\n",
            "Epoch 195/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0663\n",
            "Epoch 00195: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0664 - val_loss: 0.0748\n",
            "Epoch 196/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0667\n",
            "Epoch 00196: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0667 - val_loss: 0.0896\n",
            "Epoch 197/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0666\n",
            "Epoch 00197: val_loss did not improve from 0.06756\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0666 - val_loss: 0.0698\n",
            "Epoch 198/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0661\n",
            "Epoch 00198: val_loss improved from 0.06756 to 0.06744, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/manNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 28s 6ms/sample - loss: 0.0662 - val_loss: 0.0674\n",
            "Epoch 199/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0664\n",
            "Epoch 00199: val_loss did not improve from 0.06744\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0664 - val_loss: 0.0716\n",
            "Epoch 200/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0670\n",
            "Epoch 00200: val_loss did not improve from 0.06744\n",
            "4480/4480 [==============================] - 15s 3ms/sample - loss: 0.0670 - val_loss: 0.0757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx4-kyYbGfI",
        "colab_type": "text"
      },
      "source": [
        "## Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1iTz86KbNed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4e439f12-0491-404b-f2b5-8e5f1a7610e5"
      },
      "source": [
        "model.save_weights(os.path.join(c_path,'manNet_200.h5'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1M9sjHIbSnf",
        "colab_type": "text"
      },
      "source": [
        "## Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNSS7XIbXaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d51ff074-a81c-49c8-a3da-8064307bfc36"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(os.path.join(c_path,'{}_history.png'.format(model_name)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_gen_unet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojVYZ7Spzpv",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   Check TF version\n",
        "*    TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q4JaV2ps6z",
        "colab_type": "code",
        "outputId": "8d40f738-c7a8-4b23-ef26-0a8fb2b29448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVhQoAOqGGW",
        "colab_type": "code",
        "outputId": "11f01075-6e1c-4bd4-b5f5-623ad6dbf0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/F2O/pyF2O"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/F2O/pyF2O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vp4y0TiqKJh",
        "colab_type": "code",
        "outputId": "8fb8cd5f-26b9-469d-97b3-e0127700de4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 1.3MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692-uM-yqdaF",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9QkSiJqed6",
        "colab_type": "code",
        "outputId": "29a95f13-e382-4568-e57e-3d4a8605c143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.115.237.58:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 17916568208844195518),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10509841420619699484),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1133453659202593070),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11483983254141017286),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5754316423930606415),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7511383888336744965),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5757803457779548715),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6823495938708604983),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5819597072157920059),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12356749110173743354),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7324962667625447652)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefiHZ4qlHA",
        "colab_type": "text"
      },
      "source": [
        "# UNET generator Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwtSzOarVYM",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOro7D1krWYf",
        "colab_type": "code",
        "outputId": "4ee559bd-ea4e-4e9f-b3a9-d1ea5bea580d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import mean_absolute_error\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from core.generators import unet\n",
        "model=unet()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(lr=0.01), loss=mean_absolute_error)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "gen_enc_init_input (InputLayer) (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_1 (Conv2D)         (None, 64, 64, 64)   1792        gen_enc_init_input[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_2_act (LeakyReLU)  (None, 64, 64, 64)   0           gen_enc_conv_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_2 (Conv2D)         (None, 32, 32, 128)  73856       gen_enc_conv_2_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_2_bn (BatchNormali (None, 32, 32, 128)  512         gen_enc_conv_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_3_act (LeakyReLU)  (None, 32, 32, 128)  0           gen_enc_conv_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_3 (Conv2D)         (None, 16, 16, 256)  295168      gen_enc_conv_3_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_3_bn (BatchNormali (None, 16, 16, 256)  1024        gen_enc_conv_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_4_act (LeakyReLU)  (None, 16, 16, 256)  0           gen_enc_conv_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_4 (Conv2D)         (None, 8, 8, 512)    1180160     gen_enc_conv_4_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_4_bn (BatchNormali (None, 8, 8, 512)    2048        gen_enc_conv_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_5_act (LeakyReLU)  (None, 8, 8, 512)    0           gen_enc_conv_4_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_5 (Conv2D)         (None, 4, 4, 512)    2359808     gen_enc_conv_5_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_5_bn (BatchNormali (None, 4, 4, 512)    2048        gen_enc_conv_5[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_6_act (LeakyReLU)  (None, 4, 4, 512)    0           gen_enc_conv_5_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_6 (Conv2D)         (None, 2, 2, 512)    2359808     gen_enc_conv_6_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_6_bn (BatchNormali (None, 2, 2, 512)    2048        gen_enc_conv_6[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_7_act (LeakyReLU)  (None, 2, 2, 512)    0           gen_enc_conv_6_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_7 (Conv2D)         (None, 1, 1, 512)    2359808     gen_enc_conv_7_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_enc_conv_7_bn (BatchNormali (None, 1, 1, 512)    2048        gen_enc_conv_7[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_1_act (Activation) (None, 1, 1, 512)    0           gen_enc_conv_7_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_1 (Conv2DTranspo (None, 2, 2, 512)    2359808     gen_dec_conv_1_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_1_bn (BatchNorma (None, 2, 2, 512)    2048        gen_dec_deconv_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_1_conc (Concaten (None, 2, 2, 1024)   0           gen_dec_deconv_1_bn[0][0]        \n",
            "                                                                 gen_enc_conv_6_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_2_act (Activation) (None, 2, 2, 1024)   0           gen_dec_deconv_1_conc[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_2 (Conv2DTranspo (None, 4, 4, 512)    4719104     gen_dec_conv_2_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_2_bn (BatchNorma (None, 4, 4, 512)    2048        gen_dec_deconv_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_2_conc (Concaten (None, 4, 4, 1024)   0           gen_dec_deconv_2_bn[0][0]        \n",
            "                                                                 gen_enc_conv_5_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_3_act (Activation) (None, 4, 4, 1024)   0           gen_dec_deconv_2_conc[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_3 (Conv2DTranspo (None, 8, 8, 256)    2359552     gen_dec_conv_3_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_3_bn (BatchNorma (None, 8, 8, 256)    1024        gen_dec_deconv_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_3_conc (Concaten (None, 8, 8, 768)    0           gen_dec_deconv_3_bn[0][0]        \n",
            "                                                                 gen_enc_conv_4_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_4_act (Activation) (None, 8, 8, 768)    0           gen_dec_deconv_3_conc[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_4 (Conv2DTranspo (None, 16, 16, 128)  884864      gen_dec_conv_4_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_4_bn (BatchNorma (None, 16, 16, 128)  512         gen_dec_deconv_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_4_conc (Concaten (None, 16, 16, 384)  0           gen_dec_deconv_4_bn[0][0]        \n",
            "                                                                 gen_enc_conv_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_5_act (Activation) (None, 16, 16, 384)  0           gen_dec_deconv_4_conc[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_5 (Conv2DTranspo (None, 32, 32, 64)   221248      gen_dec_conv_5_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_5_bn (BatchNorma (None, 32, 32, 64)   256         gen_dec_deconv_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_5_conc (Concaten (None, 32, 32, 192)  0           gen_dec_deconv_5_bn[0][0]        \n",
            "                                                                 gen_enc_conv_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_6_act (Activation) (None, 32, 32, 192)  0           gen_dec_deconv_5_conc[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_6 (Conv2DTranspo (None, 64, 64, 64)   110656      gen_dec_conv_6_act[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_6_bn (BatchNorma (None, 64, 64, 64)   256         gen_dec_deconv_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_deconv_6_conc (Concaten (None, 64, 64, 128)  0           gen_dec_deconv_6_bn[0][0]        \n",
            "                                                                 gen_enc_conv_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_last_act (Activati (None, 64, 64, 128)  0           gen_dec_deconv_6_conc[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_last (Conv2DTransp (None, 128, 128, 3)  3459        gen_dec_conv_last_act[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gen_dec_conv_final_act (Activat (None, 128, 128, 3)  0           gen_dec_conv_last[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 19,304,963\n",
            "Trainable params: 19,297,027\n",
            "Non-trainable params: 7,936\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fqLlsIcrgJ6",
        "colab_type": "text"
      },
      "source": [
        "## Convert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLuMCkq5rha3",
        "colab_type": "code",
        "outputId": "e54e4931-f4d1-4788-d02a-9324fb3d4a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def convert_model_TPU(model):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "                                           strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                                               tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "                                               )\n",
        "                                           )\n",
        "\n",
        "model=convert_model_TPU(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.115.237.58:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17916568208844195518)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10509841420619699484)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1133453659202593070)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11483983254141017286)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5754316423930606415)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7511383888336744965)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5757803457779548715)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6823495938708604983)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5819597072157920059)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12356749110173743354)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7324962667625447652)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMWNlaqLNIPY",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "* Load **X_Train_ALL.h5** and **Y_Train_ALL.h5**\n",
        "* Normalize both dataset as float32\n",
        "* Split the data for **validation** and **training**\n",
        "* Drop Batch Reminders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WMdSlCZJPWi",
        "colab_type": "code",
        "outputId": "993b676a-432d-43d7-eb8b-5422f42f7852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from core.utils import readh5\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "class TRAIN_FLAGS:\n",
        "    H5_DIR   = '/content/gdrive/My Drive/PROJECTS/F2O/H5Data/' # @param\n",
        "    BATCH_SIZE      = 128  # @param\n",
        "    NUM_EPOCHS      = 200  # @param\n",
        "    IMAGE_DIM       = 128  # @param\n",
        "    X_IDEN          = 'X_Train_ALL.h5'  # @param\n",
        "    Y_IDEN          = 'Y_Train_ALL.h5'  # @param\n",
        "\n",
        "X=readh5(os.path.join(TRAIN_FLAGS.H5_DIR,TRAIN_FLAGS.X_IDEN))\n",
        "Y=readh5(os.path.join(TRAIN_FLAGS.H5_DIR,TRAIN_FLAGS.Y_IDEN))\n",
        "# Normalize\n",
        "X=X.astype('float32')/255.0\n",
        "Y=Y.astype('float32')/255.0\n",
        "# Test And Validation\n",
        "Xt,Xv,Yt,Yv=train_test_split(X,Y,test_size=0.2)\n",
        "\n",
        "print(\"Before Drop\")\n",
        "print(\"X_train: {}\".format(Xt.shape))\n",
        "print(\"Y_train: {}\".format(Yt.shape))\n",
        "print(\"X_eval: {}\".format(Xv.shape))\n",
        "print(\"Y_eval: {}\".format(Yv.shape))\n",
        "\n",
        "# Drop Batch Residue\n",
        "nb_sig_train=int(Xt.shape[0]/TRAIN_FLAGS.BATCH_SIZE)*TRAIN_FLAGS.BATCH_SIZE \n",
        "nb_sig_eval=int(Xv.shape[0]/TRAIN_FLAGS.BATCH_SIZE)*TRAIN_FLAGS.BATCH_SIZE \n",
        "\n",
        "Xt=Xt[:nb_sig_train,:,:,:]\n",
        "Yt=Yt[:nb_sig_train,:,:,:]\n",
        "Xv=Xv[:nb_sig_eval,:,:,:]\n",
        "Yv=Yv[:nb_sig_eval,:,:,:]\n",
        "\n",
        "print(\"After Drop\")\n",
        "print(\"X_train: {}\".format(Xt.shape))\n",
        "print(\"Y_train: {}\".format(Yt.shape))\n",
        "print(\"X_eval: {}\".format(Xv.shape))\n",
        "print(\"Y_eval: {}\".format(Yv.shape))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Drop\n",
            "X_train: (4480, 128, 128, 3)\n",
            "Y_train: (4480, 128, 128, 3)\n",
            "X_eval: (1120, 128, 128, 3)\n",
            "Y_eval: (1120, 128, 128, 3)\n",
            "After Drop\n",
            "X_train: (4480, 128, 128, 3)\n",
            "Y_train: (4480, 128, 128, 3)\n",
            "X_eval: (1024, 128, 128, 3)\n",
            "Y_eval: (1024, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQXx8YRNSyr",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTUQlweNYYN",
        "colab_type": "code",
        "outputId": "3e326eec-2e3c-4a10-b83d-9c6e752a5c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "\n",
        "c_path=os.path.join(os.getcwd(),'core','model_weights')\n",
        "print(c_path)\n",
        "model_name='uNet'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(c_path,'{}.h5'.format(model_name)), verbose=1, save_best_only=True)\n",
        "\n",
        "history=model.fit(Xt,Yt,validation_data=(Xv,Yv),callbacks=[checkpoint],epochs=TRAIN_FLAGS.NUM_EPOCHS,batch_size=TRAIN_FLAGS.BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights\n",
            "Train on 4480 samples, validate on 1024 samples\n",
            "Epoch 1/200\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='gen_enc_init_input_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='gen_dec_conv_final_act_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for gen_enc_init_input\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fe782424908> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.57457947731018 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.009999999776482582 {0.01}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4352/4480 [============================>.] - ETA: 1s - loss: 0.1118INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='gen_enc_init_input_10'), TensorSpec(shape=(16, 128, 128, 3), dtype=tf.float32, name='gen_dec_conv_final_act_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for gen_enc_init_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fe778a270f0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.074700832366943 secs\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.55094, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 86s 19ms/sample - loss: 0.1101 - val_loss: 0.5509\n",
            "Epoch 2/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00002: val_loss improved from 0.55094 to 0.42208, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0457 - val_loss: 0.4221\n",
            "Epoch 3/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0336\n",
            "Epoch 00003: val_loss improved from 0.42208 to 0.06123, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0334 - val_loss: 0.0612\n",
            "Epoch 4/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0298\n",
            "Epoch 00004: val_loss did not improve from 0.06123\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0298 - val_loss: 0.5628\n",
            "Epoch 5/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0292\n",
            "Epoch 00005: val_loss did not improve from 0.06123\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0292 - val_loss: 0.1190\n",
            "Epoch 6/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0285\n",
            "Epoch 00006: val_loss improved from 0.06123 to 0.04122, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0285 - val_loss: 0.0412\n",
            "Epoch 7/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00007: val_loss improved from 0.04122 to 0.02790, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0260 - val_loss: 0.0279\n",
            "Epoch 8/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00008: val_loss improved from 0.02790 to 0.02388, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0248 - val_loss: 0.0239\n",
            "Epoch 9/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0239\n",
            "Epoch 00009: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0239 - val_loss: 0.0304\n",
            "Epoch 10/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00010: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0240 - val_loss: 0.0271\n",
            "Epoch 11/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00011: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0226 - val_loss: 0.0269\n",
            "Epoch 12/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00012: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0226 - val_loss: 0.0286\n",
            "Epoch 13/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00013: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0210 - val_loss: 0.0240\n",
            "Epoch 14/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00014: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0199 - val_loss: 0.0243\n",
            "Epoch 15/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0221\n",
            "Epoch 00015: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0221 - val_loss: 0.0316\n",
            "Epoch 16/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00016: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0226 - val_loss: 0.0257\n",
            "Epoch 17/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00017: val_loss did not improve from 0.02388\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0216 - val_loss: 0.0253\n",
            "Epoch 18/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00018: val_loss improved from 0.02388 to 0.01982, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0206 - val_loss: 0.0198\n",
            "Epoch 19/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00019: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0209 - val_loss: 0.0398\n",
            "Epoch 20/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00020: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0226 - val_loss: 0.0208\n",
            "Epoch 21/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00021: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0208 - val_loss: 0.0218\n",
            "Epoch 22/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00022: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0217 - val_loss: 0.0204\n",
            "Epoch 23/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00023: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0208 - val_loss: 0.0217\n",
            "Epoch 24/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00024: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0201 - val_loss: 0.0213\n",
            "Epoch 25/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00025: val_loss did not improve from 0.01982\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0187 - val_loss: 0.0199\n",
            "Epoch 26/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00026: val_loss improved from 0.01982 to 0.01974, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0184 - val_loss: 0.0197\n",
            "Epoch 27/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00027: val_loss did not improve from 0.01974\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0180 - val_loss: 0.0205\n",
            "Epoch 28/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00028: val_loss did not improve from 0.01974\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0186 - val_loss: 0.0276\n",
            "Epoch 29/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00029: val_loss did not improve from 0.01974\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0194 - val_loss: 0.0210\n",
            "Epoch 30/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00030: val_loss did not improve from 0.01974\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0187 - val_loss: 0.0203\n",
            "Epoch 31/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00031: val_loss improved from 0.01974 to 0.01946, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0180 - val_loss: 0.0195\n",
            "Epoch 32/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00032: val_loss improved from 0.01946 to 0.01842, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0194 - val_loss: 0.0184\n",
            "Epoch 33/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00033: val_loss did not improve from 0.01842\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0179 - val_loss: 0.0219\n",
            "Epoch 34/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00034: val_loss did not improve from 0.01842\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0186 - val_loss: 0.0211\n",
            "Epoch 35/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00035: val_loss did not improve from 0.01842\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0180 - val_loss: 0.0187\n",
            "Epoch 36/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00036: val_loss did not improve from 0.01842\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0187 - val_loss: 0.0246\n",
            "Epoch 37/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00037: val_loss improved from 0.01842 to 0.01749, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0199 - val_loss: 0.0175\n",
            "Epoch 38/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00038: val_loss did not improve from 0.01749\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0195 - val_loss: 0.0184\n",
            "Epoch 39/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00039: val_loss did not improve from 0.01749\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0185 - val_loss: 0.0181\n",
            "Epoch 40/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00040: val_loss did not improve from 0.01749\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0186 - val_loss: 0.0182\n",
            "Epoch 41/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00041: val_loss did not improve from 0.01749\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0189 - val_loss: 0.0182\n",
            "Epoch 42/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00042: val_loss did not improve from 0.01749\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0185 - val_loss: 0.0197\n",
            "Epoch 43/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00043: val_loss improved from 0.01749 to 0.01596, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0171 - val_loss: 0.0160\n",
            "Epoch 44/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00044: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0170 - val_loss: 0.0169\n",
            "Epoch 45/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00045: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0186 - val_loss: 0.0206\n",
            "Epoch 46/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00046: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0176 - val_loss: 0.0195\n",
            "Epoch 47/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00047: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0176 - val_loss: 0.0196\n",
            "Epoch 48/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00048: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0186 - val_loss: 0.0189\n",
            "Epoch 49/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00049: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0179 - val_loss: 0.0170\n",
            "Epoch 50/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00050: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0172 - val_loss: 0.0210\n",
            "Epoch 51/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00051: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0178 - val_loss: 0.0193\n",
            "Epoch 52/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00052: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0163 - val_loss: 0.0184\n",
            "Epoch 53/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00053: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0184 - val_loss: 0.0187\n",
            "Epoch 54/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00054: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 9s 2ms/sample - loss: 0.0190 - val_loss: 0.0232\n",
            "Epoch 55/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00055: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0180 - val_loss: 0.0188\n",
            "Epoch 56/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0171\n",
            "Epoch 00056: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0171 - val_loss: 0.0187\n",
            "Epoch 57/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00057: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0177 - val_loss: 0.0173\n",
            "Epoch 58/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00058: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0158 - val_loss: 0.0164\n",
            "Epoch 59/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00059: val_loss did not improve from 0.01596\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0152 - val_loss: 0.0179\n",
            "Epoch 60/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00060: val_loss improved from 0.01596 to 0.01595, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0147 - val_loss: 0.0160\n",
            "Epoch 61/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0151\n",
            "Epoch 00061: val_loss improved from 0.01595 to 0.01331, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0150 - val_loss: 0.0133\n",
            "Epoch 62/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00062: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0142 - val_loss: 0.0164\n",
            "Epoch 63/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0160\n",
            "Epoch 00063: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0159 - val_loss: 0.0156\n",
            "Epoch 64/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00064: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0145 - val_loss: 0.0164\n",
            "Epoch 65/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00065: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0143 - val_loss: 0.0148\n",
            "Epoch 66/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0153\n",
            "Epoch 00066: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0152 - val_loss: 0.0146\n",
            "Epoch 67/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00067: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0142 - val_loss: 0.0141\n",
            "Epoch 68/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0150\n",
            "Epoch 00068: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0149 - val_loss: 0.0142\n",
            "Epoch 69/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00069: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0148 - val_loss: 0.0146\n",
            "Epoch 70/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0153\n",
            "Epoch 00070: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0153 - val_loss: 0.0168\n",
            "Epoch 71/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00071: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0184 - val_loss: 0.0156\n",
            "Epoch 72/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0162\n",
            "Epoch 00072: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0162 - val_loss: 0.0215\n",
            "Epoch 73/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0154\n",
            "Epoch 00073: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0154 - val_loss: 0.0167\n",
            "Epoch 74/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0135\n",
            "Epoch 00074: val_loss did not improve from 0.01331\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0135 - val_loss: 0.0145\n",
            "Epoch 75/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00075: val_loss improved from 0.01331 to 0.01313, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0148 - val_loss: 0.0131\n",
            "Epoch 76/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00076: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0152 - val_loss: 0.0183\n",
            "Epoch 77/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00077: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0157 - val_loss: 0.0178\n",
            "Epoch 78/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00078: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0172 - val_loss: 0.0253\n",
            "Epoch 79/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00079: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0180 - val_loss: 0.0143\n",
            "Epoch 80/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00080: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0170 - val_loss: 0.0156\n",
            "Epoch 81/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00081: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0158 - val_loss: 0.0148\n",
            "Epoch 82/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00082: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0146 - val_loss: 0.0157\n",
            "Epoch 83/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0141\n",
            "Epoch 00083: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0141 - val_loss: 0.0146\n",
            "Epoch 84/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0140\n",
            "Epoch 00084: val_loss did not improve from 0.01313\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0140 - val_loss: 0.0149\n",
            "Epoch 85/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0135\n",
            "Epoch 00085: val_loss improved from 0.01313 to 0.01297, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 24s 5ms/sample - loss: 0.0135 - val_loss: 0.0130\n",
            "Epoch 86/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0133\n",
            "Epoch 00086: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0133 - val_loss: 0.0140\n",
            "Epoch 87/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0133\n",
            "Epoch 00087: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0133 - val_loss: 0.0151\n",
            "Epoch 88/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0135\n",
            "Epoch 00088: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0134 - val_loss: 0.0136\n",
            "Epoch 89/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0134\n",
            "Epoch 00089: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0134 - val_loss: 0.0150\n",
            "Epoch 90/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0139\n",
            "Epoch 00090: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0139 - val_loss: 0.0148\n",
            "Epoch 91/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00091: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0146 - val_loss: 0.0175\n",
            "Epoch 92/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00092: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0143 - val_loss: 0.0140\n",
            "Epoch 93/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0127\n",
            "Epoch 00093: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0127 - val_loss: 0.0134\n",
            "Epoch 94/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0131\n",
            "Epoch 00094: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0131 - val_loss: 0.0157\n",
            "Epoch 95/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0150\n",
            "Epoch 00095: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0150 - val_loss: 0.0165\n",
            "Epoch 96/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00096: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0144 - val_loss: 0.0159\n",
            "Epoch 97/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00097: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0173 - val_loss: 0.0166\n",
            "Epoch 98/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0150\n",
            "Epoch 00098: val_loss did not improve from 0.01297\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0149 - val_loss: 0.0137\n",
            "Epoch 99/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0126\n",
            "Epoch 00099: val_loss improved from 0.01297 to 0.01284, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0126 - val_loss: 0.0128\n",
            "Epoch 100/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00100: val_loss improved from 0.01284 to 0.01280, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 24s 5ms/sample - loss: 0.0128 - val_loss: 0.0128\n",
            "Epoch 101/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00101: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0127 - val_loss: 0.0135\n",
            "Epoch 102/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0129\n",
            "Epoch 00102: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0129 - val_loss: 0.0145\n",
            "Epoch 103/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0138\n",
            "Epoch 00103: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0138 - val_loss: 0.0169\n",
            "Epoch 104/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0155\n",
            "Epoch 00104: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0154 - val_loss: 0.0215\n",
            "Epoch 105/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00105: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0170 - val_loss: 0.0146\n",
            "Epoch 106/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00106: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0147 - val_loss: 0.0230\n",
            "Epoch 107/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0141\n",
            "Epoch 00107: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0141 - val_loss: 0.0146\n",
            "Epoch 108/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00108: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0128 - val_loss: 0.0133\n",
            "Epoch 109/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0136\n",
            "Epoch 00109: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0137 - val_loss: 0.0218\n",
            "Epoch 110/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0154\n",
            "Epoch 00110: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0154 - val_loss: 0.0171\n",
            "Epoch 111/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00111: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0152 - val_loss: 0.0186\n",
            "Epoch 112/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0124\n",
            "Epoch 00112: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0124 - val_loss: 0.0134\n",
            "Epoch 113/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00113: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0128 - val_loss: 0.0141\n",
            "Epoch 114/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0118\n",
            "Epoch 00114: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0118 - val_loss: 0.0140\n",
            "Epoch 115/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0131\n",
            "Epoch 00115: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0131 - val_loss: 0.0134\n",
            "Epoch 116/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0138\n",
            "Epoch 00116: val_loss did not improve from 0.01280\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0137 - val_loss: 0.0138\n",
            "Epoch 117/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0129\n",
            "Epoch 00117: val_loss improved from 0.01280 to 0.01147, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 23s 5ms/sample - loss: 0.0128 - val_loss: 0.0115\n",
            "Epoch 118/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0118\n",
            "Epoch 00118: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 119/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0139\n",
            "Epoch 00119: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0140 - val_loss: 0.0152\n",
            "Epoch 120/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0154\n",
            "Epoch 00120: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0154 - val_loss: 0.0137\n",
            "Epoch 121/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00121: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0148 - val_loss: 0.0129\n",
            "Epoch 122/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00122: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0145 - val_loss: 0.0151\n",
            "Epoch 123/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0140\n",
            "Epoch 00123: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0141 - val_loss: 0.0161\n",
            "Epoch 124/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00124: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0147 - val_loss: 0.0202\n",
            "Epoch 125/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0153\n",
            "Epoch 00125: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0153 - val_loss: 0.0195\n",
            "Epoch 126/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0132\n",
            "Epoch 00126: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 9s 2ms/sample - loss: 0.0132 - val_loss: 0.0180\n",
            "Epoch 127/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0138\n",
            "Epoch 00127: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0138 - val_loss: 0.0159\n",
            "Epoch 128/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00128: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0146 - val_loss: 0.0143\n",
            "Epoch 129/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0141\n",
            "Epoch 00129: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 9s 2ms/sample - loss: 0.0141 - val_loss: 0.0147\n",
            "Epoch 130/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00130: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0128 - val_loss: 0.0130\n",
            "Epoch 131/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0126\n",
            "Epoch 00131: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0126 - val_loss: 0.0133\n",
            "Epoch 132/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0125\n",
            "Epoch 00132: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0125 - val_loss: 0.0118\n",
            "Epoch 133/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0127\n",
            "Epoch 00133: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0127 - val_loss: 0.0124\n",
            "Epoch 134/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0119\n",
            "Epoch 00134: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0119 - val_loss: 0.0129\n",
            "Epoch 135/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0123\n",
            "Epoch 00135: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0123 - val_loss: 0.0136\n",
            "Epoch 136/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0119\n",
            "Epoch 00136: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0119 - val_loss: 0.0126\n",
            "Epoch 137/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0123\n",
            "Epoch 00137: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0123 - val_loss: 0.0132\n",
            "Epoch 138/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00138: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0127 - val_loss: 0.0122\n",
            "Epoch 139/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0116\n",
            "Epoch 00139: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0116 - val_loss: 0.0126\n",
            "Epoch 140/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0117\n",
            "Epoch 00140: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0117 - val_loss: 0.0147\n",
            "Epoch 141/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0118\n",
            "Epoch 00141: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0118 - val_loss: 0.0142\n",
            "Epoch 142/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0127\n",
            "Epoch 00142: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0127 - val_loss: 0.0136\n",
            "Epoch 143/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0123\n",
            "Epoch 00143: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0123 - val_loss: 0.0135\n",
            "Epoch 144/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0137\n",
            "Epoch 00144: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0137 - val_loss: 0.0137\n",
            "Epoch 145/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0153\n",
            "Epoch 00145: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0154 - val_loss: 0.0144\n",
            "Epoch 146/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00146: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0157 - val_loss: 0.0131\n",
            "Epoch 147/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00147: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0146 - val_loss: 0.0205\n",
            "Epoch 148/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00148: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0145 - val_loss: 0.0191\n",
            "Epoch 149/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0124\n",
            "Epoch 00149: val_loss did not improve from 0.01147\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0124 - val_loss: 0.0130\n",
            "Epoch 150/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0117\n",
            "Epoch 00150: val_loss improved from 0.01147 to 0.01117, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 24s 5ms/sample - loss: 0.0118 - val_loss: 0.0112\n",
            "Epoch 151/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0129\n",
            "Epoch 00151: val_loss did not improve from 0.01117\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0129 - val_loss: 0.0160\n",
            "Epoch 152/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0124\n",
            "Epoch 00152: val_loss did not improve from 0.01117\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0125 - val_loss: 0.0129\n",
            "Epoch 153/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0118\n",
            "Epoch 00153: val_loss did not improve from 0.01117\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0118 - val_loss: 0.0122\n",
            "Epoch 154/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0124\n",
            "Epoch 00154: val_loss did not improve from 0.01117\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0124 - val_loss: 0.0142\n",
            "Epoch 155/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00155: val_loss did not improve from 0.01117\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0128 - val_loss: 0.0119\n",
            "Epoch 156/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0110\n",
            "Epoch 00156: val_loss improved from 0.01117 to 0.01074, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 24s 5ms/sample - loss: 0.0110 - val_loss: 0.0107\n",
            "Epoch 157/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0121\n",
            "Epoch 00157: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0121 - val_loss: 0.0198\n",
            "Epoch 158/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0124\n",
            "Epoch 00158: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0124 - val_loss: 0.0129\n",
            "Epoch 159/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0122\n",
            "Epoch 00159: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0123 - val_loss: 0.0134\n",
            "Epoch 160/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0119\n",
            "Epoch 00160: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0120 - val_loss: 0.0129\n",
            "Epoch 161/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0117\n",
            "Epoch 00161: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0117 - val_loss: 0.0121\n",
            "Epoch 162/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0118\n",
            "Epoch 00162: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0119 - val_loss: 0.0111\n",
            "Epoch 163/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0115\n",
            "Epoch 00163: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0115 - val_loss: 0.0129\n",
            "Epoch 164/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0129\n",
            "Epoch 00164: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0130 - val_loss: 0.0148\n",
            "Epoch 165/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0137\n",
            "Epoch 00165: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0136 - val_loss: 0.0173\n",
            "Epoch 166/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0127\n",
            "Epoch 00166: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0127 - val_loss: 0.0123\n",
            "Epoch 167/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0112\n",
            "Epoch 00167: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0112 - val_loss: 0.0126\n",
            "Epoch 168/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0107\n",
            "Epoch 00168: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 169/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0114\n",
            "Epoch 00169: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 170/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0111\n",
            "Epoch 00170: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0110 - val_loss: 0.0119\n",
            "Epoch 171/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0125\n",
            "Epoch 00171: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0125 - val_loss: 0.0148\n",
            "Epoch 172/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0100\n",
            "Epoch 00172: val_loss did not improve from 0.01074\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0100 - val_loss: 0.0110\n",
            "Epoch 173/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0110\n",
            "Epoch 00173: val_loss improved from 0.01074 to 0.01073, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 24s 5ms/sample - loss: 0.0110 - val_loss: 0.0107\n",
            "Epoch 174/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0102\n",
            "Epoch 00174: val_loss improved from 0.01073 to 0.00958, saving model to /content/gdrive/My Drive/PROJECTS/F2O/pyF2O/core/model_weights/uNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "4480/4480 [==============================] - 24s 5ms/sample - loss: 0.0102 - val_loss: 0.0096\n",
            "Epoch 175/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0104\n",
            "Epoch 00175: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0105 - val_loss: 0.0119\n",
            "Epoch 176/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0120\n",
            "Epoch 00176: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0121 - val_loss: 0.0115\n",
            "Epoch 177/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0108\n",
            "Epoch 00177: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 178/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0099\n",
            "Epoch 00178: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0099 - val_loss: 0.0098\n",
            "Epoch 179/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0110\n",
            "Epoch 00179: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0110 - val_loss: 0.0104\n",
            "Epoch 180/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0099\n",
            "Epoch 00180: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0099 - val_loss: 0.0105\n",
            "Epoch 181/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0106\n",
            "Epoch 00181: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0106 - val_loss: 0.0128\n",
            "Epoch 182/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0115\n",
            "Epoch 00182: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0114 - val_loss: 0.0127\n",
            "Epoch 183/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0102\n",
            "Epoch 00183: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0102 - val_loss: 0.0100\n",
            "Epoch 184/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0116\n",
            "Epoch 00184: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0116 - val_loss: 0.0130\n",
            "Epoch 185/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0134\n",
            "Epoch 00185: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0135 - val_loss: 0.0168\n",
            "Epoch 186/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0151\n",
            "Epoch 00186: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 9s 2ms/sample - loss: 0.0150 - val_loss: 0.0266\n",
            "Epoch 187/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00187: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 9s 2ms/sample - loss: 0.0142 - val_loss: 0.0126\n",
            "Epoch 188/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0122\n",
            "Epoch 00188: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0122 - val_loss: 0.0139\n",
            "Epoch 189/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0111\n",
            "Epoch 00189: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0111 - val_loss: 0.0120\n",
            "Epoch 190/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0114\n",
            "Epoch 00190: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0114 - val_loss: 0.0117\n",
            "Epoch 191/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0099\n",
            "Epoch 00191: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0100 - val_loss: 0.0103\n",
            "Epoch 192/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0109\n",
            "Epoch 00192: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0109 - val_loss: 0.0111\n",
            "Epoch 193/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0106\n",
            "Epoch 00193: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0106 - val_loss: 0.0112\n",
            "Epoch 194/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0099\n",
            "Epoch 00194: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0099 - val_loss: 0.0121\n",
            "Epoch 195/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0096\n",
            "Epoch 00195: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0096 - val_loss: 0.0112\n",
            "Epoch 196/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0100\n",
            "Epoch 00196: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0100 - val_loss: 0.0110\n",
            "Epoch 197/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0102\n",
            "Epoch 00197: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0103 - val_loss: 0.0119\n",
            "Epoch 198/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0103\n",
            "Epoch 00198: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0103 - val_loss: 0.0113\n",
            "Epoch 199/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0110\n",
            "Epoch 00199: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0110 - val_loss: 0.0114\n",
            "Epoch 200/200\n",
            "4352/4480 [============================>.] - ETA: 0s - loss: 0.0124\n",
            "Epoch 00200: val_loss did not improve from 0.00958\n",
            "4480/4480 [==============================] - 10s 2ms/sample - loss: 0.0124 - val_loss: 0.0139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx4-kyYbGfI",
        "colab_type": "text"
      },
      "source": [
        "## Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1iTz86KbNed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6a7f1386-0d16-4aea-b25b-0fa5831c48f4"
      },
      "source": [
        "model.save_weights(os.path.join(c_path,'uNet_200.h5'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1M9sjHIbSnf",
        "colab_type": "text"
      },
      "source": [
        "## Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNSS7XIbXaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b2bbefa5-33d9-4723-ed68-c9729a2024e5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(os.path.join(c_path,'{}_history.png'.format(model_name)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}